{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU 17-400/17-700 auto-graded notebook\n",
    "\n",
    "Before you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Logistic Regression & Gradient Descent Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe60cd9540617cc2e184126ec775bb7a",
     "grade": false,
     "grade_id": "collaborators",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Who did you collaborate with on this assignment? \n",
    "# if no one, collaborators should contain an empty string,\n",
    "# else list your collaborators below\n",
    "\n",
    "# collaborators = [\"\"]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dccedd0e8702e7a99d0ea08f2fa7921",
     "grade": true,
     "grade_id": "test_collaborators",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    collaborators\n",
    "except:\n",
    "    raise AssertionError(\"you did not list your collaborators, if any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click-Through Rate Prediction\n",
    "In this section you will go through the steps for creating a click-through rate (CTR) prediction pipeline. You will work with the Criteo Labs dataset.\n",
    "\n",
    "## This section will cover:\n",
    "\n",
    "* *Part 1:* Featurize categorical data using one-hot-encoding (OHE)\n",
    "\n",
    "* *Part 2:* Construct an OHE dictionary\n",
    "\n",
    "* *Part 3:* Parse CTR data and generate OHE features\n",
    " * *Visualization 1:* Feature frequency\n",
    "\n",
    "* *Part 4:* CTR prediction and logloss evaluation\n",
    " * *Visualization 2:* ROC curve\n",
    "\n",
    "* *Part 5:* Reduce feature dimension via feature hashing\n",
    "\n",
    "> Note that, for reference, you can look up the details of:\n",
    "> * the relevant Spark methods in [PySpark's DataFrame API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.DataFrame)\n",
    "> * the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN MOST LIKELY IGNORE THIS CELL. This is only of use for running this notebook locally.\n",
    "\n",
    "# THIS CELL DOES NOT NEED TO BE RUN ON DATABRICKS. \n",
    "# Note that Databricks already creates a SparkContext for you, so this cell can be skipped.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext(appName=\"hw\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"spark context started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Featurize categorical data using one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) One-hot-encoding\n",
    "\n",
    "We would like to develop code to convert categorical features to numerical ones. In order to build intuition, we will work with a unlabeled dataset with three data points, with each data point representing an animal. The first feature indicates the type of animal (bear, cat, mouse); the second feature describes the animal's color (black, tabby); and the third (optional) feature describes what the animal eats (mouse, salmon).\n",
    "\n",
    "In a one-hot-encoding (OHE) scheme, we want to represent each tuple of `(featureID, category)` via its own binary feature.  We can do this in Python by creating a dictionary that maps each tuple to a distinct integer, where the integer corresponds to a binary feature. To start, manually enter the entries in the OHE dictionary associated with the sample dataset by mapping the tuples to consecutive integers starting from zero,  ordering the tuples first by featureID and next by category.\n",
    "\n",
    "Later in this lab, we'll use OHE dictionaries to transform data points into compact lists of features that can be used in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, when a shuffle operation occurs with DataFrames, the post-shuffle partition\n",
    "# count is 200. This is controlled by Spark configuration value spark.sql.shuffle.partitions.\n",
    "# 200 is a little too high for this data set, so we set the post-shuffle partition count to\n",
    "# twice the number of available threads in Community Edition.\n",
    "sqlContext.setConf('spark.sql.shuffle.partitions', '6')  # Set default partitions for DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Data for manual OHE\n",
    "# Note: the first data point does not include any value for the optional third feature\n",
    "sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "\n",
    "def sample_to_row(sample):\n",
    "    tmp_dict = defaultdict(lambda: None)\n",
    "    tmp_dict.update(sample)\n",
    "    return [tmp_dict[i] for i in range(3)]\n",
    "\n",
    "sqlContext.createDataFrame(map(sample_to_row, [sample_one, sample_two, sample_three]),\n",
    "                           ['animal', 'color', 'food']).show()\n",
    "sample_data_df = sqlContext.createDataFrame([(sample_one,), (sample_two,), (sample_three,)], ['features'])\n",
    "sample_data_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8209ea16dbf40e73e55ee82ff619d7c1",
     "grade": false,
     "grade_id": "answer_oneHotEncoding1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Replace <FILL IN> with appropriate code\n",
    "# sample_ohe_dict_manual = {}\n",
    "# sample_ohe_dict_manual[(0, 'bear')] = <FILL IN >\n",
    "# sample_ohe_dict_manual[(0, 'cat')] = <FILL IN >\n",
    "# sample_ohe_dict_manual[(0, 'mouse')] = <FILL IN >\n",
    "# sample_ohe_dict_manual < FILL IN >\n",
    "# sample_ohe_dict_manual < FILL IN >\n",
    "# sample_ohe_dict_manual < FILL IN >\n",
    "# sample_ohe_dict_manual < FILL IN >\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6f25c59f4552f4f7f88adb22f436d4",
     "grade": true,
     "grade_id": "test_oneHotEncoding1a",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST One-hot-encoding (1a)\n",
    "assert_equal(sample_ohe_dict_manual[(0, 'bear')], 0)\n",
    "assert_equal(sample_ohe_dict_manual[(0, 'cat')], 1)\n",
    "assert_equal(sample_ohe_dict_manual[(0, 'mouse')], 2)\n",
    "assert_equal(len(sample_ohe_dict_manual.keys()), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Sparse vectors\n",
    "\n",
    "Data points can typically be represented with a small number of non-zero OHE features which are relative to the total number of features that occur in the dataset.  By leveraging this sparsity and using sparse vector representations for OHE data, we can reduce storage and computational burdens.  Below are a few sample vectors represented as dense numpy arrays.  Use [SparseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector) to represent them in a sparse fashion, and verify that both the sparse and dense representations yield the same results when computing [dot products](http://en.wikipedia.org/wiki/Dot_product) (we will later use MLlib to train classifiers via gradient descent, and MLlib will need to compute dot products between SparseVectors and dense parameter vectors).\n",
    "\n",
    "Use `SparseVector(size, *args)` to create a new sparse vector where size is the length of the vector and args are either:\n",
    "1. A list of indices and a list of values corresponding to the indices. The indices list must be sorted in ascending order. For example, SparseVector(5, [1, 3, 4], [10, 30, 40]) will represent the vector [0, 10, 0, 30, 40]. The non-zero indices are 1, 3 and 4. On the other hand, SparseVector(3, [2, 1], [5, 5]) will give you an error because the indices list [2, 1] is not in ascending order. Note: you cannot simply sort the indices list, because otherwise the values will not correspond to the respective indices anymore.\n",
    "2. A list of (index, value) pair. In this case, the indices need not be sorted. For example, SparseVector(5, [(3, 1), (1, 2)]) will give you the vector [0, 2, 0, 1, 0].\n",
    "\n",
    "SparseVectors are much more efficient when working with sparse data because they do not store zero values (only store non-zero values and their indices). You'll need to create a sparse vector representation of each dense vector `a_dense` and `b_dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f4aab6e45b9f308eac40fed53799b29",
     "grade": false,
     "grade_id": "answer_sparseVector1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# a_dense = np.array([0., 3., 0., 4.])\n",
    "# a_sparse = <FILL IN >\n",
    "\n",
    "# b_dense = np.array([0., 0., 0., 1.])\n",
    "# b_sparse = <FILL IN >\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "w = np.array([0.4, 3.1, -1.4, -.5])\n",
    "print(a_dense.dot(w))\n",
    "print(a_sparse.dot(w))\n",
    "print(b_dense.dot(w))\n",
    "print(b_sparse.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f77449dbd8d1d2dbc73e796e8fb2298e",
     "grade": true,
     "grade_id": "test_sparseVector1b",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Sparse Vectors (1b)\n",
    "assert_true(isinstance(a_sparse, SparseVector), 'a_sparse needs to be an instance of SparseVector')\n",
    "assert_true(b_dense.dot(w) == b_sparse.dot(w),\n",
    "                'dot product of b_dense and w should equal dot product of b_sparse and w')\n",
    "assert_true(a_sparse.numNonzeros() == 2, 'a_sparse should not store zero values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) OHE features as sparse vectors\n",
    "\n",
    "Now let's see how we can represent the OHE features for points in our sample dataset.  Using the mapping defined by the OHE dictionary from Part (1a), manually define OHE features for the three sample data points using SparseVector format.  In this case, all the features will have a value of 1.0.  For example, the `DenseVector` for a point with features 2 and 4 would be `[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of the sample features\n",
    "# sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "# sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3370e32345c2352fb4238ac5b882b69c",
     "grade": false,
     "grade_id": "answer_oheFeature1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code. Use SparseVector\n",
    "# sample_one_ohe_feat_manual = <FILL IN >\n",
    "# sample_two_ohe_feat_manual = <FILL IN >\n",
    "# sample_three_ohe_feat_manual = <FILL IN >\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36983ed66139e8e07fd2526ca7983d04",
     "grade": true,
     "grade_id": "test_oheFeature1c",
     "locked": true,
     "points": 11,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST OHE Features as sparse vectors (1c)\n",
    "assert_true(isinstance(sample_one_ohe_feat_manual, SparseVector),\n",
    "                'sample_one_ohe_feat_manual needs to be a SparseVector')\n",
    "assert_true(isinstance(sample_two_ohe_feat_manual, SparseVector),\n",
    "                'sample_two_ohe_feat_manual needs to be a SparseVector')\n",
    "assert_true(isinstance(sample_three_ohe_feat_manual, SparseVector),\n",
    "                'sample_three_ohe_feat_manual needs to be a SparseVector')\n",
    "\n",
    "assert_equal(sample_one_ohe_feat_manual[2], 1.0, 'incorrect value for sample_one_ohe_feat_manual')\n",
    "assert_equal(sample_one_ohe_feat_manual[3], 1.0, 'incorrect value for sample_one_ohe_feat_manual')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Define a OHE function\n",
    "\n",
    "Next we will use the OHE dictionary from Part (1a) to programatically generate OHE features from the original categorical data.  First write a function called `one_hot_encoding` that creates OHE feature vectors in `SparseVector` format.  Then use this function to create OHE features for the first sample data point and verify that the result matches the result from Part (1c).\n",
    "\n",
    "> Note: We'll pass in the OHE dictionary as a [Broadcast](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.Broadcast) variable, which will greatly improve performance when we call this function as part of a UDF. **When accessing a broadcast variable, you _must_ use `.value`.** For instance: `ohe_dict_broadcast.value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc626f1400b84737c828b47b66a67ad2",
     "grade": false,
     "grade_id": "answer_oheFunction1d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    # <FILL IN>\n",
    "    # ohe_features = [<FILL IN>]\n",
    "    # return SparseVector(<FILL IN>)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "  \n",
    "# # Calculate the number of features in sample_ohe_dict_manual\n",
    "# num_sample_ohe_feats = <FILL IN >\n",
    "# sample_ohe_dict_manual_broadcast = sc.broadcast(sample_ohe_dict_manual)\n",
    "\n",
    "# # Run one_hot_encoding() on sample_one.  Make sure to pass in the Broadcast variable.\n",
    "# sample_one_ohe_feat = <FILL IN >  \n",
    "  \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(sample_one_ohe_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cd84691fc8e6f2e773cb6e1da9d0772",
     "grade": true,
     "grade_id": "test_oheFunction1d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Define an OHE Function (1d)\n",
    "assert_true(sample_one_ohe_feat == sample_one_ohe_feat_manual,\n",
    "                'sample_one_ohe_feat should equal sample_one_ohe_feat_manual')\n",
    "assert_equal(sample_one_ohe_feat, SparseVector(7, [2, 3], [1.0, 1.0]),\n",
    "                  'incorrect value for sample_one_ohe_feat')\n",
    "assert_equal(one_hot_encoding([(1, 'black'), (0, 'mouse')], sample_ohe_dict_manual_broadcast,\n",
    "                                   num_sample_ohe_feats), SparseVector(7, [2, 3], [1.0, 1.0]),\n",
    "                  'incorrect definition for one_hot_encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) Apply OHE to a dataset\n",
    "\n",
    "Finally, use the function from Part (1d) to create OHE features for all 3 data points in the sample dataset.  You'll need to generate a [UDF](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.functions.udf) that can be used in a `DataFrame` `select` statement.\n",
    "\n",
    "> Note: Your implemenation of `ohe_udf_generator` needs to call your `one_hot_encoding` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49386eba93487b63c07d121c606c745a",
     "grade": false,
     "grade_id": "answer_oheUdfGeneratorFunction1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "def ohe_udf_generator(ohe_dict_broadcast):\n",
    "    \"\"\"Generate a UDF that is setup to one-hot-encode rows with the given dictionary.\n",
    "\n",
    "    Note:\n",
    "        We'll reuse this function to generate a UDF that can one-hot-encode rows based on a\n",
    "        one-hot-encoding dictionary built from the training data.  Also, you should calculate\n",
    "        the number of features before calling the one_hot_encoding function.\n",
    "\n",
    "    Args:\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        UserDefinedFunction: A UDF can be used in `DataFrame` `select` statement to call a\n",
    "            function on each row in a given column.  This UDF should call the one_hot_encoding\n",
    "            function with the appropriate parameters.\n",
    "    \"\"\"\n",
    "#     length = <FILL IN>\n",
    "#     return udf(lambda x: <FILL IN>, VectorUDT())\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# sample_ohe_dict_udf = ohe_udf_generator(sample_ohe_dict_manual_broadcast)\n",
    "# sample_ohe_df = sample_data_df.select( < FILL IN >)\n",
    "# sample_ohe_df.show(truncate=False)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe2dcbdb0d5e9ce5459b058ce61c7b20",
     "grade": true,
     "grade_id": "test_oheUdfGeneratorFunction1e",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Apply OHE to a dataset (1e)\n",
    "sample_ohe_data_values = sample_ohe_df.collect()\n",
    "assert_true(len(sample_ohe_data_values) == 3, 'sample_ohe_data_values should have three elements')\n",
    "assert_equal(sample_ohe_data_values[0], (SparseVector(7, {2: 1.0, 3: 1.0}),),\n",
    "                  'incorrect OHE for first sample')\n",
    "assert_equal(sample_ohe_data_values[1], (SparseVector(7, {1: 1.0, 4: 1.0, 5: 1.0}),),\n",
    "                  'incorrect OHE for second sample')\n",
    "assert_equal(sample_ohe_data_values[2], (SparseVector(7, {0: 1.0, 3: 1.0, 6: 1.0}),),\n",
    "                  'incorrect OHE for third sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Construct an OHE dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) DataFrame with rows of `(featureID, category)`\n",
    "\n",
    "To start, create a DataFrame of distinct `(feature_id, category)` tuples. In our sample dataset, the 7 items in the resulting DataFrame are `(0, 'bear')`, `(0, 'cat')`, `(0, 'mouse')`, `(1, 'black')`, `(1, 'tabby')`, `(2, 'mouse')`, `(2, 'salmon')`. Notably `'black'` appears twice in the dataset but only contributes one item to the DataFrame: `(1, 'black')`, while `'mouse'` also appears twice and contributes two items: `(0, 'mouse')` and `(2, 'mouse')`.  Use [explode](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) and [distinct](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9445adc1626d3bef10bed545a3d28ba4",
     "grade": false,
     "grade_id": "answer_dfWithRow2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import explode\n",
    "# sample_distinct_feats_df = (sample_data_df\n",
    "#                               <FILL IN>)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "sample_distinct_feats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98ef0325fd677f7c8fd669f44f04a479",
     "grade": true,
     "grade_id": "test_dfWithRow2a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST DataFrame with rows of `(featureID, category)` (2a)\n",
    "assert_equal(sorted(map(lambda r: r[0], sample_distinct_feats_df.collect())),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'incorrect value for sample_distinct_feats_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) OHE Dictionary from distinct features\n",
    "\n",
    "Next, create an RDD of key-value tuples, where each `(feature_id, category)` tuple in `sample_distinct_feats_df` is a key and the values are distinct integers ranging from 0 to (number of keys - 1).  Then convert this RDD into a dictionary, which can be done using the `collectAsMap` action.  Note that there is no unique mapping from keys to values, as all we require is that each `(featureID, category)` key be mapped to a unique integer between 0 and the number of keys.  In this exercise, any valid mapping is acceptable.  Use [zipWithIndex](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex) followed by [collectAsMap](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collectAsMap).\n",
    "\n",
    "In our sample dataset, one valid list of key-value tuples is: `[((0, 'bear'), 0), ((2, 'salmon'), 1), ((1, 'tabby'), 2), ((2, 'mouse'), 3), ((0, 'mouse'), 4), ((0, 'cat'), 5), ((1, 'black'), 6)]`. The dictionary defined in Part (1a) illustrates another valid mapping between keys and integers.\n",
    "\n",
    "> Note: We provide the code to convert the DataFrame to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca6524fe730cb4e6ebc20568f2f5a1d4",
     "grade": false,
     "grade_id": "answer_oheDict2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# sample_ohe_dict = (sample_distinct_feats_df\n",
    "#                      .rdd\n",
    "#                      .map(lambda r: tuple(r[0]))\n",
    "#                      <FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(sample_ohe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "302b11cbdc6ad6285b359fc3bf82d5ae",
     "grade": true,
     "grade_id": "test_oheDict2b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST OHE Dictionary from distinct features (2b)\n",
    "assert_equal(sorted(sample_ohe_dict.keys()),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'sample_ohe_dict has unexpected keys')\n",
    "assert_equal(sorted(sample_ohe_dict.values()), list(range(7)), 'sample_ohe_dict has unexpected values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Automated creation of an OHE dictionary\n",
    "\n",
    "Now use the code from Parts (2a) and (2b) to write a function that takes an input dataset and outputs an OHE dictionary.  Then use this function to create an OHE dictionary for the sample dataset, and verify that it matches the dictionary from Part (2b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e09487dcdeddfd148c58341b1bdf45e",
     "grade": false,
     "grade_id": "answer_createOneHotDictFunction2c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def create_one_hot_dict(input_df):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame with 'features' column): A DataFrame where each row contains a list of\n",
    "            (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "#    <FILL IN>\n",
    "#    distinct_feature_df = (<FILL IN>)\n",
    "#    key_value_tuple_list = (<FILL IN)\n",
    "#    return dict(key_value_tuple_list)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "sample_ohe_dict_auto = create_one_hot_dict(sample_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e6aef1078be6e30560f2e33fa5bc208",
     "grade": true,
     "grade_id": "test_createOneHotDictFunction2c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Automated creation of an OHE dictionary (2c)\n",
    "assert_equal(sorted(sample_ohe_dict_auto.keys()),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'sample_ohe_dict_auto has unexpected keys')\n",
    "assert_equal(sorted(sample_ohe_dict_auto.values()), list(range(7)),\n",
    "                  'sample_ohe_dict_auto has unexpected values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parse CTR data and generate OHE features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Before we can proceed, you'll first need to obtain the sample data.  ***\n",
    "\n",
    "The data is from a past [kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge/overview). The original data was too big. So we randomly sampled the data for this assignment from the original dataset.\n",
    "\n",
    "For data fileds:\n",
    "* Label - Target variable that indicates if an ad was clicked (1) or not (0).\n",
    "* I1-I13 - A total of 13 columns of integer features (mostly count features).\n",
    "* C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes.\n",
    "The semantic of the features is undisclosed.\n",
    "Just run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for getting data from the github repo.\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import Row\n",
    "url = \"https://raw.githubusercontent.com/17-700/data/master/hw3/dac.txt\"\n",
    "sc.addFile(url)\n",
    "\n",
    "raw_df = sc.textFile(\"file://\" + SparkFiles.get(\"dac.txt\")).map(lambda r: Row(r)).toDF([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Loading and splitting the data\n",
    "\n",
    "We are now ready to start working with the actual CTR data, and our first task involves splitting it into training, validation, and test sets.  Usually we use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) with the specified weights and seed to create DFs storing each of these datasets. BUT randomSplit may generate non-deterministic results. So for the sake of testing, we manually split the data in to train, validation, test by the ratio of 0.8, 0.1, 0.1.\n",
    "\n",
    "Then your work is to [cache](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cache) each of these DFs, as we will access them multiple times in the remainder of this lab. Finally, compute the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5edfe1a7841630a4bbdb3b75f15a82e3",
     "grade": false,
     "grade_id": "answer_loadSplitData3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "indexDf = raw_df.withColumn('index', f.monotonically_increasing_id())\n",
    "total_count = raw_df.count()\n",
    "train_count = int(total_count * 4 / 5)\n",
    "dev_count = int(total_count / 5)\n",
    "val_count = int(total_count / 10)\n",
    "test_count = int(total_count / 10)\n",
    "# first 20% rows\n",
    "raw_dev_df = indexDf.sort('index').limit(dev_count)\n",
    "# last 80% rows\n",
    "raw_train_df = indexDf.sort('index', ascending=False).limit(train_count).drop('index')\n",
    "# first 10% rows\n",
    "raw_validation_df = raw_dev_df.sort('index').limit(val_count).drop('index')\n",
    "# last 10% rows\n",
    "raw_test_df = raw_dev_df.sort('index', ascending=False).limit(test_count).drop('index')\n",
    "\n",
    "# # Cache and count the DataFrames\n",
    "# n_train = raw_train_df.<FILL IN>\n",
    "# n_val = raw_validation_df.<FILL IN>\n",
    "# n_test = raw_test_df.<FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(n_train, n_val, n_test, n_train + n_val + n_test)\n",
    "raw_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "889a702a433f661028204586b15e172e",
     "grade": true,
     "grade_id": "test_loadSplitData3a",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Loading and splitting the data (3a)\n",
    "assert_true(all([raw_train_df.is_cached, raw_validation_df.is_cached, raw_test_df.is_cached]),\n",
    "                'you must cache the split data')\n",
    "assert_equal(n_train, 80000, 'incorrect value for n_train')\n",
    "assert_equal(n_val, 10000, 'incorrect value for n_val')\n",
    "assert_equal(n_test, 10000, 'incorrect value for n_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Extract features\n",
    "\n",
    "We will now parse the raw training data in order to create a DataFrame that we can subsequently use to create an OHE dictionary. Note from the `show()` command in Part (3a) that each raw data point is a string containing several fields separated by some delimiters.  For now, we will ignore the first field (which is just the 0-1 label), and parse the remaining fields (or raw features).  To do this, complete the implemention of the `parse_point` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eec736bbcaddca09efa3b229451ed55d",
     "grade": false,
     "grade_id": "answer_extractFeatures3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def parse_point(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "#       <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(parse_point(raw_df.select('text').first()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d38c6262d91db01488cec1b521db53a",
     "grade": true,
     "grade_id": "test_extractFeatures3b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Extract features (3b)\n",
    "assert_equal(parse_point(raw_df.select('text').first()[0])[:3], [(0, u'0'), (1, u'127'), (2, u'1')],\n",
    "                  'incorrect implementation of parse_point')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Extracting features continued\n",
    "\n",
    "Next, we'll create a `parse_raw_df` function that creates a `label` column for the first value in a data point and a `features` column for the rest.  The `features` column will be created using `parse_point_udf`, which we've provided and is based on your `parse_point` function.  Note that to name your columns you should use [alias](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.alias).  You can split the `text` field in `raw_df` using [split](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split) and retrieve the first value of the resulting array with [getItem](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.getItem). Be sure to call [cast](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast) to cast the column value to `double`. Your `parse_raw_df` function should also cache the DataFrame it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "271458166fa96d2d7e85830296be31ea",
     "grade": false,
     "grade_id": "answer_extractFreatures3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with the appropriate code\n",
    "from pyspark.sql.functions import udf, split\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, LongType, StringType\n",
    "\n",
    "parse_point_udf = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()),\n",
    "                                                         StructField('_2', StringType())])))\n",
    "\n",
    "def parse_raw_df(raw_df):\n",
    "    \"\"\"Convert a DataFrame consisting of rows of comma separated text into labels and features.\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame with a 'text' column): DataFrame containing the raw comma separated data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with 'label' and 'features' columns.\n",
    "    \"\"\"\n",
    "#     return raw_df.select(<FILL IN>)\n",
    "#           .<FILL IN>\n",
    "#           .<FILL IN>\n",
    "#           .<FILL IN>\n",
    "#           .<FILL IN>\n",
    "#           .<FILL IN>\n",
    "#           .cache()\n",
    "\n",
    "  \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# # Parse the raw training DataFrame\n",
    "# parsed_train_df = <FILL IN>  \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "from pyspark.sql.functions import (explode, col)\n",
    "num_categories = (parsed_train_df\n",
    "                   .select(explode('features').alias('features'))\n",
    "                   .distinct()\n",
    "                   .select(col('features').getField('_1').alias('featureNumber'))\n",
    "                   .groupBy('featureNumber')\n",
    "                   .sum()\n",
    "                   .orderBy('featureNumber')\n",
    "                   .collect())\n",
    "\n",
    "print(num_categories[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "966f2aff815951fc7239a33735eea753",
     "grade": true,
     "grade_id": "test_extractFreatures3c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Extract features (3c)\n",
    "assert_true(parsed_train_df.is_cached, 'parse_raw_df should return a cached DataFrame')\n",
    "assert_equal(num_categories[2][1], 1356, 'incorrect implementation of parse_point or parse_raw_df')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Create an OHE dictionary from the dataset\n",
    "\n",
    "Note that `parse_point` returns a data point in the format of a list of `(featureID, category)` tuples, which is the same format as the sample dataset studied in Parts 1 and 2 of this lab.  Using this observation, create an OHE dictionary from the parsed training data using the function implemented in Part (2c). Note that we will assume for simplicity that all features in our CTR dataset are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "febc19848d31727c9ce0a8e0b7da729b",
     "grade": false,
     "grade_id": "answer_createOheDict3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# ctr_ohe_dict = <FILL IN>\n",
    "# num_ctr_ohe_feats = <FILL IN>\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(ctr_ohe_dict[(0, '4')])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3129635b82361eb5c8179ce0db401a4b",
     "grade": true,
     "grade_id": "test_createOheDict3d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Create an OHE dictionary from the dataset (3d)\n",
    "assert_equal(num_ctr_ohe_feats, 215556, 'incorrect number of features in ctr_ohe_dict')\n",
    "assert_true((0, '4') in ctr_ohe_dict, 'incorrect features in ctr_ohe_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Apply OHE to the dataset\n",
    "\n",
    "Now let's use this OHE dictionary, by starting with the training data that we've parsed into `label` and `features` columns, to create one-hot-encoded features.  Recall that we created a function `ohe_udf_generator` that can create the UDF that we need to convert row into `features`.  Make sure that `ohe_train_df` contains a `label` and `features` column and is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e91545c6e2840a0ca85242cef82079b",
     "grade": false,
     "grade_id": "answer_applyOhe3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with the appropriate code\n",
    "# ohe_dict_broadcast = <FILL IN>\n",
    "# ohe_dict_udf = <FILL IN>\n",
    "# ohe_train_df = (parsed_train_df\n",
    "#                   <FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(ohe_train_df.count())\n",
    "print(ohe_train_df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4681747ea5811311803a275f89205723",
     "grade": true,
     "grade_id": "test_applyOhe3e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Apply OHE to the dataset (3e)\n",
    "assert_true('label' in ohe_train_df.columns and 'features' in ohe_train_df.columns, 'ohe_train_df should have label and features columns')\n",
    "assert_true(ohe_train_df.is_cached, 'ohe_train_df should be cached')\n",
    "num_nz = sum(parsed_train_df.rdd.map(lambda r: len(r[1])).take(5))\n",
    "num_nz_alt = sum(ohe_train_df.rdd.map(lambda r: len(r[1].indices)).take(5))\n",
    "assert_equal(num_nz, num_nz_alt, 'incorrect value for ohe_train_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Feature frequency\n",
    "\n",
    "We will now visualize the number of times each of the 233,941 OHE features appears in the training data. We first compute the number of times each feature appears, then bucket the features by these counts.  The buckets are sized by powers of 2, so the first bucket corresponds to features that appear exactly once ( \\\\( \\scriptsize 2^0 \\\\) ), the second to features that appear twice ( \\\\( \\scriptsize 2^1 \\\\) ), the third to features that occur between three and four ( \\\\( \\scriptsize 2^2 \\\\) ) times, the fifth bucket is five to eight ( \\\\( \\scriptsize 2^3 \\\\) ) times and so on. The scatter plot below shows the logarithm of the bucket thresholds versus the logarithm of the number of features that have counts that fall in the buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.sql.functions import log\n",
    "\n",
    "get_indices = udf(lambda sv: list(map(int, sv.indices)), ArrayType(IntegerType()))\n",
    "feature_counts = (ohe_train_df\n",
    "                   .select(explode(get_indices('features')))\n",
    "                   .groupBy('col')\n",
    "                   .count()\n",
    "                   .withColumn('bucket', log('count').cast('int'))\n",
    "                   .groupBy('bucket')\n",
    "                   .count()\n",
    "                   .orderBy('bucket')\n",
    "                   .collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = zip(*feature_counts)\n",
    "x, y = x, np.log(y)\n",
    "\n",
    "def prepare_plot(xticks, yticks, figsize=(10.5, 6), hide_labels=False, grid_color='#999999',\n",
    "                 grid_width=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hide_labels: axis.set_ticklabels([])\n",
    "    plt.grid(color=grid_color, linewidth=grid_width, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 12, 1), np.arange(0, 14, 2))\n",
    "ax.set_xlabel(r'$\\log_e(bucketSize)$'), ax.set_ylabel(r'$\\log_e(countInBucket)$')\n",
    "plt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Handling unseen features\n",
    "\n",
    "We naturally would like to repeat the process from Part (3e), to compute OHE features for the validation and test datasets.  However, we must be careful, as some categorical values will likely appear in new data that did not exist in the training data. To deal with this situation, update the `one_hot_encoding()` function from Part (1d) to ignore previously unseen categories, and then compute OHE features for the validation data.  Remember that you can parse a raw DataFrame using `parse_raw_df`.\n",
    "> Note: you'll have to generate a new UDF using `ohe_udf_generator` so that the updated `one_hot_encoding` function is used.  And make sure to cache `ohe_validation_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "716994760bdd6599db881a44635f2c48",
     "grade": false,
     "grade_id": "answer_handelUnseenFeatures",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted, and that the\n",
    "        function handles missing features.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "#     ohe_feature = <FILL IN>\n",
    "#     return SparseVector(<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# ohe_dict_missing_udf = <FILL IN>\n",
    "# ohe_validation_df = (<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "ohe_validation_df.count()\n",
    "display(ohe_validation_df) # replace with ohe_validate_df.show() if running outside of Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abb36187be8db563de629175be61bf1f",
     "grade": true,
     "grade_id": "test_handelUnseenFeatures",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Handling unseen features (3f)\n",
    "from pyspark.sql.functions import size, sum as sqlsum\n",
    "\n",
    "assert_true(ohe_validation_df.is_cached, 'you need to cache ohe_validation_df')\n",
    "num_nz_val = (ohe_validation_df\n",
    "                .select(sqlsum(size(get_indices('features'))))\n",
    "                .first()[0])\n",
    "\n",
    "nz_expected = 367573\n",
    "assert_equal(num_nz_val, nz_expected, 'incorrect number of features: Got {0}, expected {1}'.format(num_nz_val, nz_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CTR prediction and logloss evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Logistic regression\n",
    "\n",
    "We are now ready to train our first CTR classifier.  A natural classifier for this setting is logistic regression, since it models the probability of a click-through event rather than returning a simple binary \"yes\" or \"no\". Also, when working with rare events like clicking-through, probabilistic predictions are usually more accurate.\n",
    "\n",
    "First use [LogisticRegression](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression) from the pyspark.ml package to train a model using `ohe_train_df` with a given hyperparameter configuration.  `LogisticRegression.fit` returns a [LogisticRegressionModel](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegressionModel).  Next, we'll use the `LogisticRegressionModel.coefficients` and `LogisticRegressionModel.intercept` to print out some details of the model's parameters.  Note that these are the names of the object's attributes and should be called using a syntax like `model.coefficients` for a given `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76a0dc800897f1d1dbccc40a33cdc001",
     "grade": false,
     "grade_id": "answer_logisticReg",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# Given hyperparameters\n",
    "standardization = False\n",
    "elastic_net_param = 0.0\n",
    "reg_param = .01\n",
    "max_iter = 20\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# lr = (<FILL IN>)\n",
    "\n",
    "# lr_model_basic = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('intercept: {0}'.format(lr_model_basic.intercept))\n",
    "print('length of coefficients: {0}'.format(len(lr_model_basic.coefficients)))\n",
    "sorted_coefficients = sorted(lr_model_basic.coefficients)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa5fee2e7d28fb46ee61879453be78a0",
     "grade": true,
     "grade_id": "test_logisticReg",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Logistic regression (4a)\n",
    "assert_true(np.allclose(lr_model_basic.intercept,  -1.1870497039599432, atol=1e-2), 'incorrect value for model intercept')\n",
    "assert_true(np.allclose(sorted_coefficients,\n",
    "                           [-0.10347285277044568, -0.10296978958368273, -0.10296978958368273, -0.10296978958368273, -0.10296978958368273], atol=1e-2),\n",
    "                           'incorrect value for model coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Log loss\n",
    "Throughout this lab, we will use log loss to evaluate the quality of models.  Log loss is defined as: \\\\[ \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\\\] where \\\\( \\scriptsize p\\\\) is a probability between 0 and 1 and \\\\( \\scriptsize y\\\\) is a label of either 0 or 1. Log loss is a standard evaluation criterion when predicting rare-events such as click-through rate prediction.\n",
    "\n",
    "Write a function `add_log_loss` for a DataFrame, and evaluate it on sample inputs.  This operation does not require a UDF.  You can perform a conditional branching with DataFrame columns using [when](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.when)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some example data\n",
    "example_log_loss_df = sqlContext.createDataFrame([(.5, 1), (.5, 0), (.99, 1), (.99, 0), (.01, 1),\n",
    "                                                  (.01, 0), (1., 1), (.0, 1), (1., 0)], ['p', 'label'])\n",
    "example_log_loss_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "468ac9f43fbc40fb2fe48bafcc9d25ab",
     "grade": false,
     "grade_id": "answer_logLoss4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import when, log, col\n",
    "epsilon = 1e-16\n",
    "\n",
    "def add_log_loss(df):\n",
    "    \"\"\"Computes and adds a 'log_loss' column to a DataFrame using 'p' and 'label' columns.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we add a small value (epsilon) to it and when\n",
    "        p is 1 we subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'p' and 'label' columns): A DataFrame with a probability column\n",
    "            'p' and a 'label' column that corresponds to y in the log loss formula.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with an additional column called 'log_loss'\n",
    "    \"\"\"\n",
    "#     return df.withColum(<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "add_log_loss(example_log_loss_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "874f346d650700606ab6ea71b57512fe",
     "grade": true,
     "grade_id": "test_logLoss4b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Log loss (4b)\n",
    "log_loss_values = add_log_loss(example_log_loss_df).select('log_loss').rdd.map(lambda r: r[0]).collect()\n",
    "assert_true(np.allclose(log_loss_values[:-2],\n",
    "                            [0.6931471805599451, 0.6931471805599451, 0.010050335853501338, 4.60517018598808,\n",
    "                             4.605170185988081, 0.010050335853501338, -0.0], atol=1e-2), 'log loss is not correct')\n",
    "assert_true(not(any(map(lambda x: x is None, log_loss_values[-2:]))),\n",
    "                'log loss needs to bound p away from 0 and 1 by epsilon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c)  Baseline log loss\n",
    "\n",
    "Next we will use the function we have in Part (4b) to compute the baseline log loss of the training data. A very simple yet natural baseline model is that we always make the same prediction regardless of datapoints, therefore the predicted value would be equal to the fraction of training points that correspond to click-through events (i.e., where the label is one). Compute this value (which is simply the mean of the training labels), and then use it to compute the training log loss for the baseline model.\n",
    "\n",
    "> Note: you'll need to add a `p` column to the `ohe_train_df` DataFrame so that it can be used in your function from Part (4b).  To represent a constant value as a column you can use the [lit](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lit) function to wrap the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2dc5175b1299ca363fef7c52b831f15",
     "grade": false,
     "grade_id": "answer_baselineLogLoss4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# Note that our dataset has a very high click-through rate by design\n",
    "# In practice click-through rate can be one to two orders of magnitude lower\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "# class_one_frac_train = (<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print('Training class one fraction = {0:.3f}'.format(class_one_frac_train))\n",
    "\n",
    "# log_loss_tr_base = (<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print('Baseline Train Logloss = {0:.3f}\\n'.format(log_loss_tr_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "baf209e488a24cfdf2fd4903ceb2d0ed",
     "grade": true,
     "grade_id": "test_baselineLogLoss4c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Baseline log loss (4c)\n",
    "expected_frac = 0.2339125\n",
    "expected_log_loss = 0.5439608117656105\n",
    "assert_true(np.allclose(class_one_frac_train, expected_frac, atol=1e-2), 'incorrect value for class_one_frac_train. Got {0}, expected {1}'.format(class_one_frac_train, expected_frac))\n",
    "assert_true(np.allclose(log_loss_tr_base, expected_log_loss, atol=1e-2), 'incorrect value for log_loss_tr_base. Got {0}, expected {1}'.format(log_loss_tr_base, expected_log_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Predict probability\n",
    "\n",
    "In order to compute the log loss for the model we trained in Part (4a), we need to generate predictions from this model. Write a function that computes the raw linear prediction from this logistic regression model and then passes it through a [sigmoid function](http://en.wikipedia.org/wiki/Sigmoid_function) \\\\( \\scriptsize \\sigma(t) = (1+ e^{-t})^{-1} \\\\) to return the model's probabilistic prediction. Then compute probabilistic predictions on the training data.\n",
    "\n",
    "Note that when incorporating an intercept into our predictions, we simply add the intercept to the value of the prediction obtained from the weights and features.  Alternatively, if the intercept was included as the first weight, we would need to add a corresponding feature to our data where the feature has the value one.  This is not the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db04f85c1d270a21737aa738642a0341",
     "grade": false,
     "grade_id": "answer_predictProb4d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.types import DoubleType\n",
    "from math import exp #  exp(-t) = e^-t\n",
    "\n",
    "def add_probability(df, model):\n",
    "    \"\"\"Adds a probability column ('p') to a DataFrame given a model\"\"\"\n",
    "    coefficients_broadcast = sc.broadcast(model.coefficients)\n",
    "    intercept = model.intercept\n",
    "\n",
    "    def get_p(features):\n",
    "        \"\"\"Calculate the probability for an observation given a list of features.\n",
    "\n",
    "        Note:\n",
    "            We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "        Args:\n",
    "            features: the features\n",
    "\n",
    "        Returns:\n",
    "            float: A probability between 0 and 1.\n",
    "        \"\"\"\n",
    "#         # Compute the raw value\n",
    "#         raw_prediction = <FILL IN>\n",
    "#         # Bound the raw value between 20 and -20\n",
    "#         raw_prediction = <FILL IN>\n",
    "#         # Return the probability\n",
    "#         <FILL IN>\n",
    "        \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "    get_p_udf = udf(get_p, DoubleType())\n",
    "    return df.withColumn('p', get_p_udf('features'))\n",
    "\n",
    "add_probability_model_basic = lambda df: add_probability(df, lr_model_basic)\n",
    "training_predictions = add_probability_model_basic(ohe_train_df).cache()\n",
    "\n",
    "training_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "655f867ffa29d7f76d496792238ee7ca",
     "grade": true,
     "grade_id": "test_predictProb4d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Predicted probability (4d)\n",
    "expected = 18746.356946150863\n",
    "got = training_predictions.selectExpr('sum(p)').first()[0]\n",
    "assert_true(np.allclose(got, expected, atol=1e-2),\n",
    "                'incorrect value for training_predictions. Got {0}, expected {1}'.format(got, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Evaluate the model\n",
    "\n",
    "We are now ready to evaluate the performance of the model we trained in Part (4a). To do this, first write a general function that takes a model and a DataFrame as its input, and outputs the log loss. Note that the log loss for multiple observations should be the mean of all the individual log losses. Then run this function on the OHE training data, and compare the result with the baseline log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f89174135486c5a70cc24d30bee3eb0b",
     "grade": false,
     "grade_id": "answer_evaluateModel4e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def evaluate_results(df, model, baseline=None):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Note:\n",
    "        If baseline has a value the probability should be set to baseline before\n",
    "        the log loss is calculated.  Otherwise, use add_probability to add the\n",
    "        appropriate probabilities to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'label' and 'features' columns): A DataFrame containing\n",
    "            labels and features.\n",
    "        model (LogisticRegressionModel): A trained logistic regression model. This\n",
    "            can be None if baseline is set.\n",
    "        baseline (float): A baseline probability to use for the log loss calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    \n",
    "#     with_probability_df = <FILL IN>\n",
    "#     with_log_loss_df = <FILL IN>\n",
    "#     log_loss = <FILL IN>\n",
    "#     return log_loss\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "log_loss_train_model_basic = evaluate_results(ohe_train_df, lr_model_basic)\n",
    "print('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_tr_base, log_loss_train_model_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8ee8a01b000b8df5dc15b49032bcd53",
     "grade": true,
     "grade_id": "test_evaluateModel4e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Evaluate the model (4e)\n",
    "expected_log_loss = 0.48971226940239815\n",
    "assert_true(np.allclose(log_loss_train_model_basic, expected_log_loss, atol=1e-2),\n",
    "                'incorrect value for log_loss_train_model_basic. Got {0}, expected {1}'.format(log_loss_train_model_basic, expected_log_loss))\n",
    "expected_res = 0.6931471805600546\n",
    "res = evaluate_results(ohe_train_df, None,  0.5)\n",
    "assert_true(np.allclose(res, expected_res, atol=1e-2),\n",
    "                'evaluate_results needs to handle baseline models. Got {0}, expected {1}'.format(res, expected_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Log loss of validation dataset\n",
    "\n",
    "Next, use the `evaluate_results` function and compute the log loss of validation dataset for both the baseline and logistic regression models. Notably, the baseline model for the validation dataset should still be based on the label fraction from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae5f2e641ede01f12b0c8f596601578b",
     "grade": false,
     "grade_id": "answer_valicationLogLossf4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# log_loss_val_base = <FILL IN>\n",
    "\n",
    "# log_loss_val_l_r0 = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_val_base, log_loss_val_l_r0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50502f77e9ba4cb9f6c3286536e44667",
     "grade": true,
     "grade_id": "test_valicationLogLossf4",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Validation log loss (4f)\n",
    "expected_val_base = 0.6344644324013423\n",
    "assert_true(np.allclose(log_loss_val_base, expected_val_base, atol=1e-2),\n",
    "                'incorrect value for log_loss_val_base. Got {0}, expected {1}'.format(log_loss_val_base, expected_val_base))\n",
    "expected_val_model_basic = 0.5793520014798194\n",
    "assert_true(np.allclose(log_loss_val_l_r0, expected_val_model_basic, atol=1e-2),\n",
    "                'incorrect value for log_loss_val_l_r0. Got {0}, expected {1}'.format(log_loss_val_l_r0, expected_val_model_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: ROC curve\n",
    "\n",
    "We will now visualize the performance of our model.  We generate a plot called the ROC curve.  The ROC curve shows us the trade-off between the false positive rate and true positive rate, as we liberalizing the threshold required for a positive prediction.  The performance of a random model is represented by the dashed line in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_and_scores = add_probability_model_basic(ohe_validation_df).select('label', 'p')\n",
    "labels_and_weights = labels_and_scores.collect()\n",
    "labels_and_weights.sort(key=lambda x: x[1], reverse=True)\n",
    "labels_by_weight = np.array([k for (k, v) in labels_and_weights])\n",
    "\n",
    "length = labels_by_weight.size\n",
    "true_positives = labels_by_weight.cumsum()\n",
    "num_positive = true_positives[-1]\n",
    "false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
    "\n",
    "true_positive_rate = true_positives / num_positive\n",
    "false_positive_rate = false_positives / (length - num_positive)\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(false_positive_rate, true_positive_rate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Reduce features' dimension via feature hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5a) Hash function\n",
    "\n",
    "As we just saw, using an one-hot-encoding featurization can yield a model with good statistical accuracy.  However, the number of distinct categories across all features is quite large -- recall that we observed 233K categories in the training data in Part (3c).  Moreover, the full training dataset includes more than 33M distinct categories, and the training dataset itself is just a small subset of labeled data in real world.  Hence, featurizing via an one-hot-encoding representation could lead to a very large feature vector. To reduce the dimensionality of the feature space, we will use feature hashing.\n",
    "\n",
    "Below is a hash function that we will use for this part of the lab.  We will first use this hash function with the three sample data points from Part (1a) to gain some intuition.  Implement the following code to hash those three sample points using two different values for `numBuckets` and observe the resulting hashed feature dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import functools \n",
    "import hashlib\n",
    "\n",
    "def hash_function(raw_feats, num_buckets, print_mapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        num_buckets (int): Number of buckets to use as features.\n",
    "        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = { category + ':' + str(ind):\n",
    "                int(int(hashlib.md5((category + ':' + str(ind)).encode('utf-8')).hexdigest(), 16) % num_buckets)\n",
    "                for ind, category in raw_feats}\n",
    "    if(print_mapping): print(mapping)\n",
    "\n",
    "    def map_update(l, r):\n",
    "        l[r] += 1.0\n",
    "        return l\n",
    "\n",
    "    sparse_features = functools.reduce(map_update, mapping.values(), defaultdict(float))\n",
    "    return dict(sparse_features)\n",
    "\n",
    "# Reminder of the sample values:\n",
    "# sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "# sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8766444e4b11b332433d342420e59f6c",
     "grade": false,
     "grade_id": "answer_hashFunction5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# # Use four buckets\n",
    "# samp_one_four_buckets = hash_function(sample_one, < FILL IN >, True)\n",
    "# samp_two_four_buckets = hash_function(sample_two, < FILL IN >, True)\n",
    "# samp_three_four_buckets = hash_function(sample_three, < FILL IN >, True)\n",
    "\n",
    "# # Use one hundred buckets\n",
    "# samp_one_hundred_buckets = hash_function(sample_one, < FILL IN >, True)\n",
    "# samp_two_hundred_buckets = hash_function(sample_two, < FILL IN >, True)\n",
    "# samp_three_hundred_buckets = hash_function(sample_three, < FILL IN >, True)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('\\n\\t\\t 4 Buckets \\t\\t\\t 100 Buckets')\n",
    "print('Sample One:\\t {0}\\t\\t\\t {1}'.format(samp_one_four_buckets, samp_one_hundred_buckets))\n",
    "print('Sample Two:\\t {0}\\t\\t {1}'.format(samp_two_four_buckets, samp_two_hundred_buckets))\n",
    "print('Sample Three:\\t {0}\\t {1}'.format(samp_three_four_buckets, samp_three_hundred_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3066187788b5f62a8f6ff9c566cf6fe3",
     "grade": true,
     "grade_id": "test_hashFunction5a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Hash function (5a)\n",
    "assert_equal(samp_one_four_buckets, {3: 2.0}, 'incorrect value for samp_one_four_buckets')\n",
    "assert_equal(samp_three_hundred_buckets, {80: 1.0, 82: 1.0, 51: 1.0},\n",
    "                  'incorrect value for samp_three_hundred_buckets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5b) Create hashed features\n",
    "\n",
    "Next, we will use this hash function to create hashed features for our CTR datasets. Use the given UDF to create a function that takes in a DataFrame and returns both labels and hashed features.  Then use it to create new training, validation and test datasets with hashed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfa9920b4a741c7bf013eb6d054ab25f",
     "grade": false,
     "grade_id": "answer_createHashFeatures5b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.ml.linalg import Vectors\n",
    "num_hash_buckets = 2 ** 15\n",
    "\n",
    "# UDF that returns a vector of hashed features given an Array of tuples\n",
    "tuples_to_hash_features_udf = udf(lambda x: Vectors.sparse(num_hash_buckets, hash_function(x, num_hash_buckets)), VectorUDT())\n",
    "\n",
    "def add_hashed_features(df):\n",
    "    \"\"\"Return a DataFrame with labels and hashed features.\n",
    "\n",
    "    Note:\n",
    "        Make sure you cache the DataFrame that you are returning.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'label' and 'features' column): A DataFrame containing labels and the features to be hashed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n",
    "            SparseVector of hashed features.\n",
    "    \"\"\"\n",
    "#     return     <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# hash_train_df = <FILL IN>\n",
    "# hash_validation_df = <FILL IN>\n",
    "# hash_test_df = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "hash_train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c40949b19e413b0afe05bb6344587b3",
     "grade": true,
     "grade_id": "test_createHashFeatures5b",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Creating hashed features (5b)\n",
    "hash_train_df_feature_sum = sum(hash_train_df\n",
    "                                  .rdd\n",
    "                                  .map(lambda r: sum(r[1].indices))\n",
    "                                  .take(10))\n",
    "hash_validation_df_feature_sum = sum(hash_validation_df\n",
    "                                       .rdd\n",
    "                                       .map(lambda r: sum(r[1].indices))\n",
    "                                       .take(10))\n",
    "hash_test_df_feature_sum = sum(hash_test_df\n",
    "                                 .rdd\n",
    "                                 .map(lambda r: sum(r[1].indices))\n",
    "                                 .take(10))\n",
    "\n",
    "expected_train_sum = 6333443\n",
    "assert_equal(hash_train_df_feature_sum, expected_train_sum,\n",
    "                  'incorrect number of features in hash_train_df. Got {0}, expected {1}'.format(hash_train_df_feature_sum, expected_train_sum))\n",
    "\n",
    "expected_validation_sum = 6340030\n",
    "assert_equal(hash_validation_df_feature_sum, expected_validation_sum,\n",
    "                  'incorrect number of features in hash_validation_df. Got {0}, expected {1}'.format(hash_validation_df_feature_sum, expected_validation_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5c) Sparsity\n",
    "\n",
    "Since we only have 33K hashed features versus 233K OHE features, we could expect our OHE features to be sparser. Verify this hypothesis by computing the average sparsity of the OHE and the hashed training datasets.\n",
    "\n",
    "Note that if you have a `SparseVector` named `sparse`, calling `len(sparse)` returns the total number of features, not the number features with entries.  `SparseVector` objects have the attributes `indices` and `values` that contain information about non-zero features.  Continuing with our example, these can be accessed using `sparse.indices` and `sparse.values`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e37c6bfd27cb45276768bf5e34bcf734",
     "grade": false,
     "grade_id": "answer_sparsity5c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def vector_feature_sparsity(sparse_vector):\n",
    "    \"\"\"Calculates the sparsity of a SparseVector.\n",
    "\n",
    "    Args:\n",
    "        sparse_vector (SparseVector): The vector containing the features.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of features found in the vector to the total number of features.\n",
    "    \"\"\"\n",
    "#     return     <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "a_sparse_vector = Vectors.sparse(5, {0: 1.0, 3: 1.0})\n",
    "a_sparse_vector_sparsity = vector_feature_sparsity(a_sparse_vector)\n",
    "print('This vector should have sparsity 2/5 or .4.')\n",
    "print('Sparsity = {0:.2f}.'.format(a_sparse_vector_sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6ca86abd9beaf7b1e0456ec9b863b47",
     "grade": true,
     "grade_id": "test_sparsity5c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Sparsity (5c)\n",
    "assert_equal(a_sparse_vector_sparsity, .4,\n",
    "                'incorrect value for a_sparse_vector_sparsity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5d) Sparsity continued\n",
    "\n",
    "Now we have a function to calculate vector sparsity, we'll wrap it up in a UDF and apply it to the entire DataFrame to obtain the average sparsity of features in that DataFrame.  We'll use this function to calculate the average sparsity of both the one-hot-encoded training DataFrame and  the hashed training DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "046048a2b4ed5b35f55c1f78623d1da2",
     "grade": false,
     "grade_id": "answer_sparsity5d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "feature_sparsity_udf = udf(vector_feature_sparsity, DoubleType())\n",
    "\n",
    "def get_sparsity(df):\n",
    "    \"\"\"Calculates the average sparsity for the features in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'features' column): A DataFrame with sparse features.\n",
    "\n",
    "    Returns:\n",
    "        float: The average feature sparsity.\n",
    "    \"\"\"\n",
    "#     return (<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# average_sparsity_ohe = <FILL IN>\n",
    "# average_sparsity_hash = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('Average OHE Sparsity: {0:.7e}'.format(average_sparsity_ohe))\n",
    "print('Average Hash Sparsity: {0:.7e}'.format(average_sparsity_hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44b1e0512bb3ed807ee64aaf3bebc950",
     "grade": true,
     "grade_id": "test_sparsity5d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Sparsity (5d)\n",
    "expected_ohe = 1.8092746e-04\n",
    "assert_true(np.allclose(average_sparsity_ohe, expected_ohe, atol=1e-2),\n",
    "                'incorrect value for average_sparsity_ohe. Got {0}, expected {1}'.format(average_sparsity_ohe, expected_ohe))\n",
    "expected_hash = 1.1895943e-03\n",
    "assert_true(np.allclose(average_sparsity_hash, expected_hash, atol=1e-2),\n",
    "                'incorrect value for average_sparsity_hash. Got {0}, expected {1}'.format(average_sparsity_hash, expected_hash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5e) Logistic model with hashed features\n",
    "\n",
    "Now let's train a logistic regression model using the hashed training features. Use the given hyperparameters, train and fit the model, then evaluate the log loss on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef61ef1ac18a7f3e504f51741403bb12",
     "grade": false,
     "grade_id": "answer_modelWithHashFeature5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# Given hyperparameters\n",
    "standardization = False\n",
    "elastic_net_param = 0.7\n",
    "reg_param = .001\n",
    "max_iter = 20\n",
    "\n",
    "# lr_hash = (<FILL IN>)\n",
    "\n",
    "# lr_model_hashed = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('intercept: {0}'.format(lr_model_hashed.intercept))\n",
    "print(len(lr_model_hashed.coefficients))\n",
    "\n",
    "log_loss_train_model_hashed = evaluate_results(hash_train_df, lr_model_hashed)\n",
    "print(('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\thashed = {1:.3f}'\n",
    "       .format(log_loss_tr_base, log_loss_train_model_hashed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdae0b218a20d5acd5842141dc17b06b",
     "grade": true,
     "grade_id": "test_modelWithHashFeature5e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Logistic model with hashed features (5e)\n",
    "expected =  0.481478172974873\n",
    "assert_true(np.allclose(log_loss_train_model_hashed, expected, atol=1e-2),\n",
    "                'incorrect value for log_loss_train_model_hashed. Got {0}, expected {1}'.format(log_loss_train_model_hashed, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5f) Evaluate the performance on the test set\n",
    "\n",
    "Finally, we will evaluate the model from Part (5e) on the test set.  Compare the resulting log loss with the baseline log loss on the test set, which can be computed in the same way where the validation log loss was computed in Part (4f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8824f00fa7475804eaa2b607e4e30f62",
     "grade": false,
     "grade_id": "answer_evaluateOnTestset5f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# # Log loss for the best model from (5e)\n",
    "# log_loss_test = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# ## Log loss for the baseline model\n",
    "# class_one_frac_test = <FILL IN>\n",
    "# print('Class one fraction for test data: {0}'.format(class_one_frac_test))\n",
    "# log_loss_test_baseline = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(('Hashed Features Test Log Loss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_test_baseline, log_loss_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2181a1e0646dc9acbe4c171311ba9a7",
     "grade": true,
     "grade_id": "test_evaluateOnTestset5f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Evaluate on the test set (5f)\n",
    "expected_test_baseline = 0.6257336255412367\n",
    "assert_true(np.allclose(log_loss_test_baseline, expected_test_baseline, atol=1e-2),\n",
    "                'incorrect value for log_loss_test_baseline. Got {0}, expected {1}'.format(log_loss_test_baseline, expected_test_baseline))\n",
    "expected_test = 0.5748571343610652\n",
    "assert_true(np.allclose(log_loss_test, expected_test, atol=1e-2),\n",
    "                'incorrect value for log_loss_test. Got {0}, expected {1}'.format(log_loss_test, expected_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization\n",
    "\n",
    "In this part we will explore building a distributed version of minibatch SGD along with two other stochastic gradient descent optimization algorithms. Although the data and model that we will use to illustrate these concepts may be relatively simple, these techniques should generalize well to larger data scales along with more sophisticated models such as deep neural networks.\n",
    "\n",
    "Our [dataset](https://archive.ics.uci.edu/ml/datasets/BlogFeedback) originates from blog posts. The raw contents of the blog posts were crawled and processed. Our taks to predict the number of comments of a blog post in the upcoming 24 hours based on its attributes combined with additional features derived from blog posts published in the last 72 hours. We will be using a small subset of this data. Read [\"Feedback Prediction for Blogs\"](http://www.cs.bme.hu/~buza/pdfs/gfkl2012_blogs.pdf) for more information about these kind of analyses.\n",
    "\n",
    "\n",
    "## This section will cover:\n",
    "\n",
    "*  *Part 1:* Loading and Parsing the Dataset\n",
    "*  *Part 2:* Minibatch SGD\n",
    "*  *Part 3:* Agagrad\n",
    "*  *Part 4:* Adaptive Moment Estimation (Adam)\n",
    "\n",
    "> Note that, for reference, you can look up the details of:\n",
    "> * the relevant Spark methods in [PySpark's DataFrame API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.DataFrame)\n",
    "> * the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Parsing the Dataset\n",
    "\n",
    "### Exercise 1(a)\n",
    "\n",
    "Let's start by first loading the dataset and examining the number of observations and fields in this dataset. We can use the DataFrame [count method](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count) to check how many observations we have, and [fieldNames](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/types.html#StructType.fieldNames) on the DataFrame's `struct` field to get the field names. Use [first](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first) to get the first row of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/17-700/data/master/hw3/blogData.csv\"\n",
    "sc.addFile(url)\n",
    "\n",
    "blog_df = sqlContext.read.csv(\"file://\" + SparkFiles.get(\"blogData.csv\"), inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9053d38df5d8d7eddb8a3604be41234d",
     "grade": false,
     "grade_id": "answer_loadPosts",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "# num_points = <FILL IN>\n",
    "# print (num_points)\n",
    "# field_names = <FILL IN>\n",
    "# sample_point = <FILL IN>\n",
    "# print (sample_points)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe1d7a6fbc9bc032524c4b0dacf32c2b",
     "grade": true,
     "grade_id": "test_loadPosts",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(num_points, 4000, \"incorrect value for num_points\")\n",
    "assert_equal(field_names, [\"_c{}\".format(i) for i in range(281)], \"incorrect value for field_names\")\n",
    "assert_equal([40.30467, 53.845657, 0.0, 401.0, 15.0, 15.52416, 32.44188, 0.0, 377.0, 3.0, 14.044226, 32.615417, 0.0, 377.0, 2.0, 34.567566, 48.475178, 0.0, 378.0, 12.0, 1.4799345, 46.18691, -356.0, 377.0, 0.0, 1.0761671, 1.795416, 0.0, 11.0, 0.0, 0.4004914, 1.0780969, 0.0, 9.0, 0.0, 0.37755936, 1.07421, 0.0, 9.0, 0.0, 0.972973, 1.704671, 0.0, 10.0, 0.0, 0.022932023, 1.521174, -8.0, 9.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "            [x for x in sample_point])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to prepare the data for machine learning. Since all of our data is numeric, this should be a simple task. Our target value (the number of comments in the next 24 hours) is contained in the last field of the dataframe. We can use a [VectorAssembler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) and its [transform](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Transformer.transform) method to convert the remaining fields into a feature vector.\n",
    "\n",
    "### Exercise 1(b)\n",
    "\n",
    "* Rename the field corresponding to the target field to \"label\"\n",
    "* Convert the remaining fields into a feature vector using a [VectorAssembler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler)\n",
    "* Use the [take](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) method to extract the first 5 lines of the vectorized Dataframe and print each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65226b66701084a2905cf32790a2491a",
     "grade": false,
     "grade_id": "answer_vectorizePosts",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines and replace <FILL_IN> with the appropriate code\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# vectorizer = VectorAssembler()\n",
    "# vectorizer.setInputCols(<FILL_IN>)\n",
    "# vectorizer.setOutputCol(<FILL_IN>)\n",
    "# vectorized_df = vectorizer.transform(<FILL IN>).<FILL_IN>\n",
    "# sample_points = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "for sample_point in sample_points:\n",
    "    print(sample_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ebef4fb9126fb826b5b3936c3a2d66b",
     "grade": true,
     "grade_id": "test_vectorizePosts",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(vectorized_df.schema.fieldNames(), [\"features\", \"label\"], \"incorrect schema for vectorized dataframe\")\n",
    "assert_equal(len(sample_points), 5, \"incorrect length for sample_points\")\n",
    "assert_equal(vectorized_df.first().label, 1.0, \"incorrect label for first row of vectorized dataframe\")\n",
    "assert_equal(len(list(filter(lambda x: x != 0, [x for x in sample_points[0].features]))), 43,\n",
    "             \"incorrect feature length for first sample_points entry\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our sample points, we observe that our feature vectors seem rather sparse (we could use the `get_sparsity` method we derived in the previous homework section to verify this). Another way of dealing with this sparsity besides the techniques used in the previous section is to use [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) as a dimensionality reduction tool. Spark ML makes this straightforward for us to apply through the [PCA](https://spark.apache.org/docs/latest/ml-features.html#pca) transformation.\n",
    "\n",
    "### Exercise 1(c)\n",
    "\n",
    "* Use PCA to project the feature vectors down to a 20-dimensional space.\n",
    "* Use the [take](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) method to extract the first 5 lines of the vectorized Dataframe and print each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98ebe7afd33e5be796feab3a90036c09",
     "grade": false,
     "grade_id": "answer_pcaPosts",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines and replace <FILL_IN> with the appropriate code\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# pca = <FILL_IN>\n",
    "# pca_df = pca.fit(<FILL_IN>).transform(<FILL IN>).<FILL_IN>\n",
    "# sample_pca_points = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "for sample_pca_point in sample_pca_points:\n",
    "    print(sample_pca_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e9bc09a0036026905dfcd00f016f62b",
     "grade": true,
     "grade_id": "test_pcaPosts",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(pca_df.schema.fieldNames(), [\"features\", \"label\"], \"incorrect schema for PCA dataframe\")\n",
    "assert_equal(len(sample_pca_points), 5, \"incorrect length for sample_pca_points\")\n",
    "assert_equal(1.0, pca_df.first().label, \"incorrect label for first row of PCA dataframe\")\n",
    "assert_true(np.allclose(sample_pca_points[0].features.array,\n",
    "                        np.array([-3.23313766e+01, -8.32128388e+02, -1.06007278e+02, -4.05039027e+02,\n",
    "                                  8.51011986e+00,  5.24211169e+00, -9.93285358e+00, -7.12650327e+00,\n",
    "                                  -1.86928731e+00, -3.45218111e+01,  6.76336167e+00, -1.02970993e+00,\n",
    "                                  -3.18646624e-02, -1.66425793e-01,  9.44414030e-01, -3.65068755e-01,\n",
    "                                  6.39599245e-01, -8.74570077e-01, -3.96366118e-01, -2.54296755e-01]), atol=1e-2),\n",
    "                        \"incorrect feature for first sample_pca_point entry\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(d)\n",
    "\n",
    "Now that our feature space is reduced, let's scale our feature vectors, which should improve the stability and speed up the convergence of our gradient descent algorithms. Once again, Spark ML provides a useful [StandardScaler](https://spark.apache.org/docs/latest/ml-features.html#standardscaler) method that makes this process straightforward.\n",
    "\n",
    "Finally, we'll also append a bias term of 1 to our feature vectors so that we can calculate an intercept term when performing linear regression.\n",
    "\n",
    "* Implement `scale_dataframe` by using [StandardScaler](https://spark.apache.org/docs/latest/ml-features.html#standardscaler) configured with `withMean = True` to normalize the feature vectors\n",
    "* Also append a bias term of 1 to each feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b039271f8f299813dd7249e208c7128b",
     "grade": false,
     "grade_id": "answer_scaling",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "def scale_dataframe(dataframe):\n",
    "    \"\"\"Calculate the fraction of variance explained by the top `k` eigenvectors.\n",
    "\n",
    "    Args:\n",
    "        dataframe: a dataframe containing a \"features\" field to normalize and a \"label\" field\n",
    "        \n",
    "    Returns:\n",
    "        dataframe: the dataframe with a normalized \"features\" field appended with a bias term with the original \"label\" field\n",
    "    \"\"\"\n",
    "    # scaler = StandardScaler(<FILL_IN>)\n",
    "    # scaled_df = scaler.fit(<FILL_IN>).transform(<FILL_IN>).<FILL_IN>\n",
    "    # df_with_bias = <FILL_IN>\n",
    "    # return <FILL IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "example_data = sqlContext.createDataFrame([(Vectors.dense(1, 2, 3), 1),\n",
    "                                           (Vectors.dense(3, 1, 2), 2),\n",
    "                                           (Vectors.dense(2, 3, 1), 3)], [\"features\", \"label\"])\n",
    "scale_dataframe(example_data).show(truncate=False)\n",
    "scale_dataframe(pca_df).first().features.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88e626f58006e75519e4cb1c29cbbb4e",
     "grade": true,
     "grade_id": "test_scaling",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scaled_features = [row['features'].array for row in scale_dataframe(example_data).collect()]\n",
    "assert_true(np.allclose(scaled_features, np.array([[-1.,  0.,  1.,  1.], [ 1., -1.,  0.,  1.], [ 0.,  1., -1.,  1.]]), atol=1e-2),\n",
    "                        \"incorrect feature for scaled example data\")\n",
    "\n",
    "assert_true(np.allclose(scale_dataframe(pca_df).first().features.array,\n",
    "                        np.array([ 0.71003709,  0.43869673, -0.42505435, -1.16038263,  0.00869739,\n",
    "                                  -0.02160378,  1.1597074 ,  1.04233605, -0.20461856, -0.39373467,\n",
    "                                  -0.1819323 , -0.83310861,  0.23984184, -0.018611  ,  0.10128653,\n",
    "                                  -0.692232  ,  1.11750318, -1.06069006, -0.94329821, -0.47101456,\n",
    "                                  1.        ]), atol=1e-2),\n",
    "                        \"incorrect feature for first entry of scaled pca dataframe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Minibatch SGD\n",
    "\n",
    "As we explored in Homework 2, gradient descent is an optimization algorithm used to minimize a cost function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. Although it can work well, the method we implemented in the previous homework involves computing the gradient over the entire dataset before updating the weights at each iteration, which can be prohibitive when dealing with large datasets where the cost of each independent variable iteration becomes O(n).\n",
    "\n",
    "Stochastic gradient descent instead uses the cost gradient of a single random example at each iteration before updating the weights, which allows us to quickly make progress and oftentimes converge more rapidly in practice at the cost of noisier error updates. Minibatch SGD falls somewhere in between both extremes, where a cost gradient calculated on a batch of examples is used to update the model weights at each step. It also provides attractive efficiency properties as further detailed [here](http://d2l.ai/chapter_optimization/minibatch-sgd.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(a)\n",
    "\n",
    "Let's start by first evaluating minibatch SGD using the feature vectors that we've reduced using PCA. We'll split our data using an 80/20 train and test split. We'll also cache our splits to speed up evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f365e98d545c54fd4e9b60a10fdb4fda",
     "grade": false,
     "grade_id": "answer_pcaSplit",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "def get_train_test_split(dataframe):\n",
    "    index_df = dataframe.withColumn('index', f.monotonically_increasing_id())\n",
    "    split100 = index_df.count()\n",
    "    split20 = int(split100/5)\n",
    "    split80 = split100 - split20\n",
    "    \n",
    "    # take first 20% rows\n",
    "    test_df = index_df.sort('index').limit(split20).drop('index')\n",
    "    \n",
    "    # take last 80% rows\n",
    "    train_df = index_df.sort('index', ascending=False).limit(split80).drop('index')\n",
    "    \n",
    "    return train_df, test_df\n",
    "    \n",
    "train_pca_df, test_pca_df = get_train_test_split(scale_dataframe(pca_df))\n",
    "\n",
    "# Cache and count the DataFrames\n",
    "# n_pca_train = <FILL IN>\n",
    "# n_pca_test = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(n_pca_train, n_pca_test, n_pca_train + n_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4868a58e4c1857f4fb2924e4a3877eeb",
     "grade": true,
     "grade_id": "test_pcaSplit",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Loading and splitting the data (3a)\n",
    "assert_true(all([train_pca_df.is_cached, test_pca_df.is_cached]),\n",
    "                'you must cache the split pca data')\n",
    "assert_equal(n_pca_train, 3200, 'incorrect value for n_pca_train')\n",
    "assert_equal(n_pca_test, 800, 'incorrect value for n_pca_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(b)\n",
    "\n",
    "Our implementation of minibatch SGD will be similar to the implementation of gradient descent in Homework 2 with a few changes to make the training process more efficient. One improvement that we can make is to calculate the gradient and the loss of the data point evaluated in an iteration over a single pass of the dataset. Remember that we can predict by computing the dot product between weights and an observation's features, and that the gradient summand can be written \\\\( \\scriptsize (\\mathbf{w}^\\top \\mathbf{x} - y) \\mathbf{x}\\\\). We can return the squared loss for each evaluated data point so that we can calculate the root mean squared error in a later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c7eeb2072a90059b1526fda576c94e8",
     "grade": false,
     "grade_id": "answer_gradientAndLoss",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "def gradient_and_loss(weights, data_point):\n",
    "    \"\"\"Calculates the gradient summand and loss for a given weight and data point.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        weights (DenseVector): An array of model weights (betas).\n",
    "        data_point (features, label): A single observation consisting of a feature vector and a label.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A (gradient, loss) tuple. The gradient summand (DenseVector) should be of the same\n",
    "        length as `weights.`\n",
    "    \"\"\"\n",
    "    # prediction = <FILL_IN>\n",
    "    # loss = <FILL_IN>\n",
    "    # gradient_summand = <FILL_IN>\n",
    "    # return gradient_summand, loss\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "example_w = DenseVector([1, 1, 1])\n",
    "# gradient_summand = (dot([1 1 1], [3 1 4]) - 2) * [3 1 4] = (8 - 2) * [3 1 4] = [18 6 24]\n",
    "# loss = dot([1 1 1], [3 1 4])^2 = (8 - 2)^2 = 36\n",
    "example_data = sqlContext.createDataFrame([(Vectors.dense(3, 1, 4), 2.0)], [\"features\", \"label\"])\n",
    "\n",
    "print(example_data.rdd.map(lambda x: gradient_and_loss(example_w, x)).first())\n",
    "\n",
    "example_w = DenseVector([.24, 1.2, -1.4])\n",
    "example_data = sqlContext.createDataFrame([(Vectors.dense(-1.4, 4.2, 2.1), 3.0)], [\"features\", \"label\"])\n",
    "\n",
    "print(example_data.rdd.map(lambda x: gradient_and_loss(example_w, x)).first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d097cbba22ddcba68e05380dbfa3350c",
     "grade": true,
     "grade_id": "test_gradientAndLoss",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "example_w = DenseVector([1, 1, 1])\n",
    "example_data = sqlContext.createDataFrame([(Vectors.dense(3, 1, 4), 2.0)], [\"features\", \"label\"])\n",
    "summand_one, loss_one = example_data.rdd.map(lambda x: gradient_and_loss(example_w, x)).first()\n",
    "assert_true(np.allclose(summand_one, [18., 6., 24.], atol=1e-2), 'incorrect value for summand_one')\n",
    "assert_equal(loss_one, 36.0, 'incorrect value for loss_one')\n",
    "\n",
    "example_w = DenseVector([.24, 1.2, -1.4])\n",
    "example_data = sqlContext.createDataFrame([(Vectors.dense(-1.4, 4.2, 2.1), 3.0)], [\"features\", \"label\"])\n",
    "summand_two, loss_two = example_data.rdd.map(lambda x: gradient_and_loss(example_w, x)).first()\n",
    "assert_true(np.allclose(summand_two, [1.7304, -5.1912, -2.5956], atol=1e-2), 'incorrect value for summand_two')\n",
    "assert_equal(loss_two, 1.5276960000000006, 'incorrect value for loss_two')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(c)\n",
    "\n",
    "We will now define the aggregation that we will call in the inner loop of our minibatch SGD algorithm to get the summed gradient and squared loss in a single dataset pass. Implement `seq_op` and `comb_op` such that the aggregation returns a tuple of form `(total_gradient, total_square_loss, batch_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "132c2a3a8dfb32a9fd357b4e79f0b800",
     "grade": false,
     "grade_id": "answer_minibatchAgg",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def seq_op(weights, aggregrated, observation):\n",
    "    \"\"\"\n",
    "    Calculates the gradient and squared loss for a single data point and adds it to the running accumulated result.\n",
    "    Args:\n",
    "        weights (broadcast of DenseVector): broadcast of the weights vector\n",
    "        aggregated (DenseVector, double, int): tuple containing the running sum of the gradient, squared loss,\n",
    "        and number of data points evaluated so far\n",
    "    Returns:\n",
    "        (DenseVector, double, int): updated running sum of the gradient, squared loss, and number of data points\n",
    "        evaluated so far\n",
    "    \"\"\"\n",
    "    # return <FILL_IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def comb_op(aggregationA, aggregationB):\n",
    "    \"\"\"\n",
    "    Combines the running accumulated result in two partitions into a single result.\n",
    "    Args:\n",
    "        aggregationA (DenseVector, double, int): tuple containing the running sum of the gradient, squared loss,\n",
    "        and number of data points evaluated so far in a partition\n",
    "        aggregationB (DenseVector, double, int): tuple containing the running sum of the gradient, squared loss,\n",
    "        and number of data points evaluated so far in a partition\n",
    "    Returns:\n",
    "        (DenseVector, double, int): combined sum\n",
    "    \"\"\"\n",
    "    # return <FILL_IN>\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "weights_broadcast = sc.broadcast(DenseVector([1, 1, 1]))\n",
    "example_data = sqlContext.createDataFrame([(Vectors.dense(3, 1, 4), 1.0),\n",
    "                                          (Vectors.dense(2, 1, 5), 2.0),\n",
    "                                          (Vectors.dense(4, 1, 7), 3.0),\n",
    "                                          (Vectors.dense(4, 1, 0), 4.0)], [\"features\", \"label\"])\n",
    "\n",
    "print(example_data.rdd.aggregate((np.zeros(3), 0.0, 0), lambda x, y: seq_op(weights_broadcast, x, y), comb_op))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80e57eb49da126f00d619692a2477a52",
     "grade": true,
     "grade_id": "test_minibatchAgg",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gradient, loss, count = example_data.rdd.aggregate((np.zeros(3), 0.0, 0), lambda x, y: seq_op(weights_broadcast, x, y), comb_op)\n",
    "assert_true(np.allclose(gradient, [73.,  23., 121.], atol=1e-2), 'incorrect value for gradient')\n",
    "assert_equal(loss, 167.0, 'incorrect value for loss')\n",
    "assert_equal(count, 4, 'incorrect value for count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(d)\n",
    "\n",
    "We're now ready to put everything together and construct our minibatch SGD algorithm! At each iteration, we will sample a fraction of our dataset and compute the summed loss and gradient of our selected minibatch using the aggregation function we previously defined. Our update rule is similar to that we defined in Homework 2 for gradient except that we now divide the gradient sum used to update the weights by our batch size instead of the total number of observations in the training set.\n",
    "\n",
    "* Sample the training set without replacement, using the specified minibatch fraction and using the zero-indexed iteration number as the random seed\n",
    "* Calcuate the root mean squared loss as the training error at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "738535c4f9a27f7ff573fda4800f7807",
     "grade": false,
     "grade_id": "answer_minibatchSGD",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "def linreg_minibatch_sgd(train_data, minibatch_fraction, num_iters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with minibatch SGD.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        train_data (RDD of (features, vector)): The labeled data for use in training the model.\n",
    "        minibatch_fraction (float): fraction of the dataset to use for each minibatch\n",
    "        num_iters (int): The number of iterations of gradient descent to perform.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The number of features in the training data\n",
    "    d = len(train_data.first().features)\n",
    "    w = np.zeros(d)\n",
    "    alpha = 0.1\n",
    "    \n",
    "    # We will compute and store the training error after each iteration\n",
    "    error_train = np.zeros(num_iters)\n",
    "\n",
    "#     for i in range(num_iters):\n",
    "#         weights_broadcast = <FILL_IN> # Broadcast the current weights vector\n",
    "#         # Get a minibatch using the sample method. Don't use replacement and use the current iteration as the seed\n",
    "#         train_batch = <FILL_IN>\n",
    "        \n",
    "#         # Get the gradient, loss and batch size\n",
    "#         gradient, loss, batch_size = train_batch.aggregate(<FILL_IN>)\n",
    "#         batch_gradient = <FILL_IN>\n",
    "       \n",
    "#         # Calculate the root mean squared error for this iteration \n",
    "#         error_train[i] = <FILL_IN>\n",
    "\n",
    "#         # Update the weights\n",
    "#         alpha_i = alpha / np.sqrt(i+1)\n",
    "#         w -= alpha_i * batch_gradient\n",
    "    \n",
    "#     return w, error_train\n",
    "  \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# create a toy dataset with n = 10, d = 3, and then run 5 iterations of minibatch SGD\n",
    "# note: the resulting model will not be useful; the goal here is to verify that\n",
    "# linreg_minibatch_sgd is working properly\n",
    "example_n = 20\n",
    "example_d = 3\n",
    "\n",
    "truncate = udf(lambda v: Vectors.dense(list(v)[:example_d]), VectorUDT())\n",
    "example_data = train_pca_df.limit(example_n).select(truncate(\"features\").alias(\"features\"), \"label\")\n",
    "\n",
    "example_data.show(2, truncate=False)\n",
    "example_num_iters = 5\n",
    "example_weights, example_error_train = linreg_minibatch_sgd(example_data.rdd, 0.5, example_num_iters)\n",
    "print (example_weights)\n",
    "print (example_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e5d20ba0b603917e8a8c3bb63a964b5",
     "grade": true,
     "grade_id": "test_minibatchSGD",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(np.allclose(example_weights, [-3.58685976, 3.96030386, -0.46859036], atol=1e-2), 'incorrect value for example_weights')\n",
    "assert_true(np.allclose(example_error_train, [14.550405, 28.330522, 13.57563332, 9.83343336, 11.02731603], atol=1e-2),\n",
    "            'incorrect value for example_error_train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(e)\n",
    "\n",
    "Now let's train our linear regression model on all of our training data and evaluate its accuracy on the test set (this might take a few minutes). One (but not the only) way of getting the test set RMSE is to reuse the aggregate function that we previously defined to get the total squared loss on the test set. Although the gradient calculations are unneccessary, it shouldn't matter too much for our purposes.\n",
    "\n",
    "* Run minibatch SGD with a minibatch fraction of 0.2 against the entire training set for 100 iterations\n",
    "* Calculate the test RMSE at after training is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0bd234720cba77f81e33ad11f8407b0",
     "grade": false,
     "grade_id": "answer_minibatchSGD_train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "\n",
    "num_iters = 100\n",
    "minibatch_fraction = 0.2\n",
    "\n",
    "# minibatch_weights, minibatch_error_train = linreg_minibatch_sgd(<FILL_IN>)\n",
    "# _, minibatch_test_loss, _ = test_pca_df.rdd.aggregate(<FILL_IN>)\n",
    "# minibatch_test_rmse = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(minibatch_weights)\n",
    "print(minibatch_error_train)\n",
    "print(minibatch_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5df9005e63d9a75b0e90c2d3ac00182",
     "grade": true,
     "grade_id": "test_minibatchSGD_train",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(np.allclose(minibatch_weights, np.array([-1.58843999, -5.72548809,  2.05269507,  3.07769617,  9.04400967,\n",
    "                                                     5.49920315,  2.69786858,  3.41055626, -0.41300786,  0.6864585 ,\n",
    "                                                     -0.24572472, -0.43805686,  1.47487744,  0.03779474, -1.12206336,\n",
    "                                                     0.634931  , -0.83604169,  0.348343  ,  0.61246073, -0.73123336,\n",
    "                                                     9.5519935 ]), atol=1e-2), 'incorrect value for minibatch_weights')\n",
    "assert_equal(len(minibatch_error_train), 100, 'incorrect length for minibatch_error_train')\n",
    "assert_true(np.allclose(minibatch_test_rmse, 13.832791135070456, atol=1e-2), 'incorrect value for minibatch_test_rmse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Minibatch Train Error\n",
    "\n",
    "We can visualize the evolution of the training error against the number of iterations. Compared to gradient descent, we can see how progress is indeed \"noisier.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = prepare_plot(np.arange(0, num_iters, 10), np.arange(17, 35))\n",
    "ax.set_ylim(17, 35)\n",
    "plt.plot(range(num_iters), minibatch_error_train)\n",
    "ax.set_xticklabels(map(str, range(0, num_iters, 10)))\n",
    "ax.set_xlabel('Iteration'), ax.set_ylabel(r'Training Error')\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(f)\n",
    "\n",
    "Finally, let's explore how minibatch SGD performs against our original dataset (before we reduced the feature dimensionality with PCA).\n",
    "\n",
    "* Create new scaled train_df and test_df datasets from `vectorized_df` using the previously defined `get_train_test_split` and `scale_dataframe functions`\n",
    "* Cache the new train and test dataframes\n",
    "* Run minibatch SGD with a minibatch fraction of 0.2 against the entire training set for 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b04e390790a9543fd7d4b83de2f6abce",
     "grade": false,
     "grade_id": "answer_minibatchSGD_nopca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "\n",
    "num_iters = 10\n",
    "minibatch_fraction = 0.2\n",
    "\n",
    "# train_df, test_df = <FILL_IN>\n",
    "# n_train = <FILL_IN>\n",
    "# n_test = <FILL_IN>\n",
    "# minibatch_full_weights, minibatch_full_train = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(n_train, n_test, n_train + n_test)\n",
    "print(minibatch_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfcafe1bfbbdb948a6131d2895ec39fb",
     "grade": true,
     "grade_id": "test_minibatchSGD_nopca",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(all([train_df.is_cached, test_df.is_cached]),\n",
    "                'you must cache the split data')\n",
    "assert_equal(n_train, 3200, 'incorrect value for n_train')\n",
    "assert_equal(n_test, 800, 'incorrect value for n_test')\n",
    "assert_equal(len(train_df.first().features), 281, 'incorrect feature length in train_df')\n",
    "assert_equal(len(test_df.first().features), 281, 'incorrect feature length in test_df')\n",
    "assert_equal(len(minibatch_full_weights), 281, 'incorrect length for minibatch_full_weights')\n",
    "\n",
    "assert_true(np.allclose(minibatch_full_train, np.array([3.09847438e+01, 3.60587988e+01, 5.20752518e+01, 8.45603279e+01,\n",
    "                                                     4.04831699e+02, 2.42182783e+03, 7.90918727e+03, 2.38528959e+04,\n",
    "                                                     6.66401140e+04, 1.68829245e+05]), atol=1e-2),\n",
    "            'incorrect value for minibatch_full_train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: Minibatch Train Error when using all Features\n",
    "\n",
    "We can visualize the evolution of the training error (in log scale) against the number of iterations. We see that our training error is now actually rising (it will grow to infinity with more iterations). There are several ways of conditioning gradient descent to make it more stable with sparse data, some of which we will explore in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = prepare_plot(np.arange(num_iters), np.arange(3, 14))\n",
    "ax.set_ylim(3, 14)\n",
    "plt.plot(range(num_iters), np.log(minibatch_full_train))\n",
    "ax.set_xticklabels(map(str, range(num_iters)))\n",
    "ax.set_xlabel('Iteration'), ax.set_ylabel(r'log(Training Error)')\n",
    "display(fig)\n",
    "print(minibatch_full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Adagrad\n",
    "\n",
    "[AdaGrad](https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) (for adaptive gradient algorithm) is a modified stochastic gradient descent algorithm developed in 2011 with per-parameter learning rates. Intuitively, it increases the learning rate for sparser parameters and decreases the rate for ones that are more frequent. For this reason, it is well-suited for dealing with sparse data and improves the robustness of SGD in these scenarios. Examples of such applications include natural language processing and image recognition. [Jeff Dean et al](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf) used it to train large-scale neural networks at Google, and, among other things, teach them to [recognize cats in Youtube videos](https://www.wired.com/2012/06/google-x-neural-network/).\n",
    "\n",
    "The modifications needed to adapt our existing gradient descend method to use Adagrad are fairly simple. Similar to minibatch SGD, we update our weight vector at each iteration by using the gradient and a learning rate parameter. However, we now also divide the general learning rate at iteration t for every parameter **<sub>*i*</sub> based on the past gradients that have been computed for **<sub>*i*</sub>. More formally, our update rule becomes:\n",
    "\n",
    "\\\\( \\mathbf{g}_t = \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t}(\\mathbf{w}^\\top \\mathbf{x} - y) \\mathbf{x}\\\\)\\\n",
    "\\\\( \\mathbf{s}_t = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2\\\\)\\\n",
    "\\\\( \\mathbf{w}_t = \\mathbf{w}_{t-1} - \\frac{\\alpha}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\cdot \\mathbf{g}_t.\\\\)\n",
    "\n",
    "Where |B| is our mini-batch size, ** is our configured learning rate and ** is a small term used to avoid dividing by zero.\n",
    "\n",
    "See [here](http://d2l.ai/chapter_optimization/adagrad.html) for more details and background.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3(a)\n",
    "\n",
    "Let's start by writing our new update function. Fill out the below function to return new weights and parameter update weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "989fba9d0e0d142c4d4078b63a8fc020",
     "grade": false,
     "grade_id": "answer_updateAdagrad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "\n",
    "epsilon = 1e-8\n",
    "alpha = 1\n",
    "\n",
    "def update_weights_adagrad(prev_w, prev_s, gradient):\n",
    "    \"\"\"Calculates the weights and parameter update weights specified by the Adagrad update step.\n",
    "    Args:\n",
    "        prev_w (DenseVector): The previous model weights\n",
    "        prev_s (DenseVector): The previous parameter update weights\n",
    "        gradient (DenseVector): Gradient for this iteration\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (new_weights, new_parameter_update_weights).\n",
    "    \"\"\"\n",
    "    # s = <FILL_IN>\n",
    "    # w = <FILL_IN>\n",
    "    # return w, s\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "test_gradient = Vectors.dense([1, 3, 2])\n",
    "test_w = [10, 8, 9]\n",
    "test_s = [1, 4, 9]\n",
    "\n",
    "w, s = update_weights_adagrad(test_w, test_s, test_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88dba5418a2b42534698e1464d1279d1",
     "grade": true,
     "grade_id": "test_updateAdagrad",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(np.allclose(w, np.array([9.29289322, 7.16794971, 8.4452998]), atol=1e-2), 'incorrect value for w')\n",
    "assert_true(np.allclose(s.array, np.array([2.0, 13.0, 13.0]), atol=1e-2), 'incorrect value for s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3(b)\n",
    "\n",
    "We can now train using Adagrad by reusing most of what that we wrote to implement minibatch SGD. The main difference is that we will now use the function we implemented in the previous exercise to update the weights during each iteration. Complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1d3d1dd44387ef48b789636324a47c1",
     "grade": false,
     "grade_id": "answer_adagrad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "def linreg_adagrad(train_data, minibatch_fraction, num_iters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with Adagrad.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        train_data (RDD of (features, vector)): The labeled data for use in training the model.\n",
    "        minibatch_fraction (float): fraction of the dataset to use for each minibatch\n",
    "        num_iters (int): The number of iterations of gradient descent to perform.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The number of features in the training data\n",
    "    d = len(train_data.first().features)\n",
    "    w = np.zeros(d)\n",
    "    s = np.zeros(d)\n",
    "    \n",
    "    # We will compute and store the training error after each iteration\n",
    "    error_train = np.zeros(num_iters)\n",
    "\n",
    "#     for i in range(num_iters):\n",
    "#         weights_broadcast = <FILL_IN> # Broadcast the current weights vector\n",
    "#         # Get a minibatch using the sample method. Don't use replacement and use the current iteration as the seed\n",
    "#         train_batch = <FILL_IN>\n",
    "        \n",
    "#         # Get the gradient, loss and batch size\n",
    "#         gradient, loss, batch_size = train_batch.aggregate(<FILL_IN>)\n",
    "#         batch_gradient = <FILL_IN>\n",
    "        \n",
    "#         # Calculate the root mean squared error for this iteration \n",
    "#         error_train[i] = <FILL_IN>\n",
    "\n",
    "#         # Update the weights\n",
    "#         w, s = update_weights_adagrad(<FILL_IN>)\n",
    "    \n",
    "#     return w, error_train\n",
    "  \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# create a toy dataset with n = 10, d = 3, and then run 5 iterations of minibatch SGD\n",
    "# note: the resulting model will not be useful; the goal here is to verify that\n",
    "# linreg_adagrad is working properly\n",
    "example_n = 20\n",
    "example_d = 3\n",
    "\n",
    "truncate = udf(lambda v: Vectors.dense(list(v)[:example_d]), VectorUDT())\n",
    "example_data = train_pca_df.limit(example_n).select(truncate(\"features\").alias(\"features\"), \"label\")\n",
    "\n",
    "example_data.show(2, truncate=False)\n",
    "example_num_iters = 5\n",
    "example_weights, example_error_train = linreg_adagrad(example_data.rdd, 0.5, example_num_iters)\n",
    "print (example_weights)\n",
    "print (example_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f8f51176faf54666c922efea5779efe",
     "grade": true,
     "grade_id": "test_adagrad",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(np.allclose(example_weights, [-2.03703969, 2.25568569, -1.7138502], atol=1e-2), 'incorrect value for example_weights')\n",
    "assert_true(np.allclose(example_error_train, [14.550405, 30.85333793, 10.46677688, 8.48714209, 11.35877978], atol=1e-2),\n",
    "            'incorrect value for example_error_train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3(c)\n",
    "\n",
    "Let's try to once again train using our original training dataset without feature reduction (this will once again take a while).\n",
    "\n",
    "* Run gradient descent with Adagrad with a minibatch fraction of 0.2 against the entire training set for 100 iterations\n",
    "* Calculate the test RMSE at after training is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "848a9c52a2e93f01fbfb9efb057f054f",
     "grade": false,
     "grade_id": "answer_adagrad_train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "\n",
    "num_iters = 100\n",
    "minibatch_fraction = 0.2\n",
    "\n",
    "# adagrad_weights, adagrad_error_train = linreg_adagrad(<FILL_IN>)\n",
    "# _, adagrad_test_loss, _ = test_df.rdd.aggregate(<FILL_IN>)\n",
    "# adagrad_test_rmse = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(adagrad_weights)\n",
    "print(adagrad_error_train)\n",
    "print(adagrad_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec82334d80ec727b56fe4f3f49c47631",
     "grade": true,
     "grade_id": "test_adagrad_train",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(adagrad_weights), 281, 'incorrect length for adagrad_weights')\n",
    "assert_equal(len(adagrad_error_train), 100, 'incorrect length for adagrad_error_train')\n",
    "assert_true(np.allclose(adagrad_test_rmse, 14.350947808742774, atol=1e-2), 'incorrect value for adagrad_test_rmse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 3: Adagrad Train Error\n",
    "\n",
    "Let's visualize the evolution of the training error against the number of iterations. We should be able to see the training error properly decrease as in our first visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = prepare_plot(np.arange(0, num_iters, 10), np.arange(17, 67, 10))\n",
    "ax.set_ylim(17, 67)\n",
    "plt.plot(range(num_iters), adagrad_error_train)\n",
    "ax.set_xticklabels(map(str, range(0, num_iters, 10)))\n",
    "ax.set_xlabel('Iteration'), ax.set_ylabel(r'Training Error')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 4: Adagrad Feature Weights\n",
    "\n",
    "One way of exploring how Adagrad handles our data sparsity is to look at the magnitude of the trained feature weigths. In the plot below, we sort the feature weights by magnitude and display the top 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_count = 20\n",
    "top_indices = np.argsort(np.abs(adagrad_weights))[-top_count:][::-1]\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.bar([str(x) for x in top_indices], np.abs(adagrad_weights[top_indices]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Adaptive Moment Estimation (Adam)\n",
    "\n",
    "[Adaptive Moment Estimation (Adam)](https://arxiv.org/pdf/1412.6980.pdf) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients as in Adagrad, Adam also keeps an exponentially decaying average of past gradients, similar to momentum. Adam has become a popular choice in machine learning as one of the more robust and effecitve optimization algorithms, especially in deep learning.\n",
    "\n",
    "One of the main ideas behind Adam is to uses exponential weighted moving averages (also known as leaky averaging) to obtain an estimate of both the momentum and also the second moment of the gradient. More formally, it uses the state variables:\n",
    "\n",
    "\\\\( \\mathbf{v}_t = \\beta_1 \\mathbf{v}_{t-1} + (1 - \\beta_1) \\mathbf{g}_t\\\\)\\\n",
    "\\\\( \\mathbf{s}_t = \\beta_2 \\mathbf{s}_{t-1} + (1 - \\beta_2) \\mathbf{g}_t^2.\\\\)\n",
    "\n",
    "Where *g* is defined as in Adagrad and **<sub>1</sub> and **<sub>2</sub> are chosen nonnegative weighting parameters. As *v*<sub>*t*</sub> abd *s*<sub>*t*</sub> are initialized to zero, they are biased toward zero during the initial time steps, especially when the decay rates **<sub>1</sub> and **<sub>2</sub> are small. To counteract this, we normalize the state variables as:\n",
    "\n",
    "\\\\( \\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_1^t}\\\\)\\\n",
    "\\\\( \\hat{\\mathbf{s}}_t = \\frac{\\mathbf{s}_t}{1 - \\beta_2^t}.\\\\)\n",
    "\n",
    "Our update rule at every step then becomes:\n",
    "\n",
    "\\\\( \\mathbf{w}_t = \\mathbf{w}_{t-1} - \\frac{\\alpha \\hat{\\mathbf{v}}_t}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon}\\\\)\n",
    "\n",
    "\n",
    "See [here](http://d2l.ai/chapter_optimization/adam.html) for more details and background.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4(a)\n",
    "\n",
    "Let's start by writing the update for Adam. Fill out the below function to return new weights and parameter update weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6d678a91db42b795f26254807703fc8",
     "grade": false,
     "grade_id": "answer_updateAdam",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "\n",
    "epsilon = 10e-6\n",
    "alpha = 1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "def update_weights_adam(prev_w, prev_v, prev_s, gradient, t):\n",
    "    \"\"\"Calculates the weights and parameter update weights specified by the Adagrad update step.\n",
    "    Args:\n",
    "        prev_w (DenseVector): The previous model weights\n",
    "        prev_v (DenseVector): The previous parameter update weights\n",
    "        prev_s (DenseVector): The previous parameter update weights\n",
    "        gradient (DenseVector): Gradient for this iteration\n",
    "        t (int): iteration number (zero-indexed)\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (new_weights, new_v, new_s).\n",
    "    \"\"\"\n",
    "#     v = <FILL_IN>\n",
    "#     s = <FILL_IN>\n",
    "#     v_hat = <FILL_IN>\n",
    "#     s_hat = <FILL_IN>\n",
    "#     w = <FILL_IN>\n",
    "#     return w, v, s\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "test_gradient = Vectors.dense([1, 3, 2])\n",
    "test_w = Vectors.dense([10, 8, 9])\n",
    "test_v = Vectors.dense([2, 5, 3])\n",
    "test_s = Vectors.dense([1, 4, 9])\n",
    "\n",
    "w, v, s = update_weights_adam(test_w, test_v, test_s, test_gradient, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa1ec947ba4ac191aaaf487c1b6703ee",
     "grade": true,
     "grade_id": "test_updateAdam",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(np.allclose(w, np.array([9.6162, 7.5155, 8.8047]), atol=1e-2), 'incorrect value for w')\n",
    "assert_true(np.allclose(v.array, np.array([1.9, 4.8, 2.9]), atol=1e-2), 'incorrect value for v')\n",
    "assert_true(np.allclose(s.array, np.array([1.0, 4.005, 8.995]), atol=1e-2), 'incorrect value for s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4(b)\n",
    "\n",
    "As with Adagrad, we can train using Adam by reusing most of what we wrote in previous sections. Once again, the main difference is that we will now use the function we implemented in the previous exercise to update the weights during each iteration. Complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f2bc0320a909d2472e4b615e5d2de69",
     "grade": false,
     "grade_id": "answer_adam",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "def linreg_adam(train_data, minibatch_fraction, num_iters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with Adam.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        train_data (RDD of (features, vector)): The labeled data for use in training the model.\n",
    "        minibatch_fraction (float): fraction of the dataset to use for each minibatch\n",
    "        num_iters (int): The number of iterations of gradient descent to perform.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The number of features in the training data\n",
    "    d = len(train_data.first().features)\n",
    "    w = np.zeros(d)\n",
    "    v = np.zeros(d)\n",
    "    s = np.zeros(d)\n",
    "    \n",
    "    # We will compute and store the training error after each iteration\n",
    "    error_train = np.zeros(num_iters)\n",
    "\n",
    "#     for i in range(num_iters):\n",
    "#         weights_broadcast = <FILL_IN> # Broadcast the current weights vector\n",
    "#         # Get a minibatch using the sample method. Don't use replacement and use the current iteration as the seed\n",
    "#         train_batch = <FILL_IN>\n",
    "        \n",
    "#         # Get the gradient, loss and batch size\n",
    "#         gradient, loss, batch_size = train_batch.aggregate(<FILL_IN>)\n",
    "#         batch_gradient = <FILL_IN>\n",
    "        \n",
    "#         # Calculate the root mean squared error for this iteration \n",
    "#         error_train[i] = <FILL_IN>\n",
    "\n",
    "#         # Update the weights\n",
    "#         w, v, s = update_weights_adam(<FILL_IN>)\n",
    "    \n",
    "#     return w, error_train\n",
    "  \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# create a toy dataset with n = 10, d = 3, and then run 5 iterations of minibatch SGD\n",
    "# note: the resulting model will not be useful; the goal here is to verify that\n",
    "# linreg_adam is working properly\n",
    "example_n = 20\n",
    "example_d = 3\n",
    "\n",
    "truncate = udf(lambda v: Vectors.dense(list(v)[:example_d]), VectorUDT())\n",
    "example_data = train_pca_df.limit(example_n).select(truncate(\"features\").alias(\"features\"), \"label\")\n",
    "\n",
    "example_data.show(2, truncate=False)\n",
    "example_num_iters = 5\n",
    "example_weights, example_error_train = linreg_adam(example_data.rdd, 0.5, example_num_iters)\n",
    "print (example_weights)\n",
    "print (example_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cebd0223c60cd7b67bf48678dbc055c4",
     "grade": true,
     "grade_id": "test_adam",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(np.allclose(example_weights, [-3.5072271, 3.97331401, -2.83269863], atol=1e-2), 'incorrect value for example_weights')\n",
    "assert_true(np.allclose(example_error_train, [14.550405, 30.85335126, 10.40982057, 8.19053844, 11.06324425], atol=1e-2),\n",
    "            'incorrect value for example_error_train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3(c)\n",
    "\n",
    "Let's train using our original training dataset without feature reduction one more time (this will once again take a while).\n",
    "\n",
    "* Run gradient descent with Adam with a minibatch fraction of 0.2 against the entire training set for 100 iterations\n",
    "* Calculate the test RMSE at after training is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49c46314e4f92c31940ff4b8ee6f8006",
     "grade": false,
     "grade_id": "answer_adam_train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "\n",
    "num_iters = 100\n",
    "minibatch_fraction = 0.2\n",
    "\n",
    "# adam_weights, adam_error_train = linreg_adam(<FILL_IN>)\n",
    "# _, adam_test_loss, _ = test_df.rdd.aggregate(<FILL_IN>)\n",
    "# adam_test_rmse = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(adam_weights)\n",
    "print(adam_error_train)\n",
    "print(adam_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f991a5814733f763d5b8bc162aafc2c",
     "grade": true,
     "grade_id": "test_adam_train",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(adam_weights), 281, 'incorrect length for adagrad_weights')\n",
    "assert_equal(len(adam_error_train), 100, 'incorrect length for adam_error_train')\n",
    "assert_true(np.allclose(adam_test_rmse, 14.24153151766628, atol=1e-2), 'incorrect value for adam_test_rmse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 5: Adam Train Error\n",
    "\n",
    "Let's plot once the train error once again. It's natural to see some noisiness at the beginning, but training error should eventually decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = prepare_plot(np.arange(0, num_iters, 10), np.arange(17, 97, 10))\n",
    "ax.set_ylim(17, 97)\n",
    "plt.plot(range(num_iters), adam_error_train)\n",
    "ax.set_xticklabels(map(str, range(0, num_iters, 10)))\n",
    "ax.set_xlabel('Iteration'), ax.set_ylabel(r'Training Error')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 6: Adam Feature Weights\n",
    "\n",
    "As with Adagrad, we plot the top 20 feature weights sorted by magnitude. This feature set should have significant overlap with Adagrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_count = 20\n",
    "top_indices = np.argsort(np.abs(adam_weights))[-top_count:][::-1]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar([str(x) for x in top_indices], np.abs(adam_weights[top_indices]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Graded Question (10 points)\n",
    "(Enter your answer into the cell below)\n",
    "\n",
    "If you had to recommend one of these approaches for use on a large dataset, which do you believe would be the best choice and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit (also manually graded, up to 20 points)\n",
    "\n",
    "#### Part 1 (5 points):\n",
    "(Enter your answer into the cell below)\n",
    "\n",
    "Explain a situation where Adagrad performs better than Adam (or vice-versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (15 points):\n",
    "(Enter your answer into the cell below)\n",
    "\n",
    "Build a reproducible example demonstrating a scenario where one approach performs better than the other. You may use example data from [Kaggle](https://www.kaggle.com/) or the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/index.php) or from somewhere else entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you're now hopefully more familiar with some of more widely deployed optimization algorithms :) Although these techniques may seem slightly overkill when performing linear regression on the datsets we've examined, they still represent the state of the art when working with more non-trivial optimization problems, especially in deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "CTR_DF_answers",
  "notebookId": 743615222353704
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
