{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU 17-400/17-700 auto-graded notebook\n",
    "\n",
    "Before you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CMU 17400/17700 Data Science and Machine Learning at Scale\n",
    "\n",
    "## Homework 1: Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a67bf091ef18740219d46e761e609e33",
     "grade": false,
     "grade_id": "collaborators",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Who did you collaborate with on this assignment? \n",
    "# if no one, collaborators should contain an empty string,\n",
    "# else list your collaborators below\n",
    "\n",
    "# collaborators = [\"\"]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "602e4d64f0a700e809c9ac4ec497d087",
     "grade": true,
     "grade_id": "test_collaborators",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    collaborators\n",
    "except:\n",
    "    raise AssertionError(\"you did not list your collaborators, if any\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** 1. Wikipedia**\n",
    "\n",
    "In this exercise, you will familiarize yourself with Spark by exploring full-text Wikipedia articles. The volume of unstructured text in existence is growing dramatically, and Spark is an excellent tool for analyzing this type of data.\n",
    "\n",
    "Gauging how popular a programming language is important for companies judging whether or not they should adopt an emerging programming language. For that reason, industry analyst firm RedMonk has bi-annually computed a ranking of programming language popularity using a variety of data sources, typically from websites like GitHub and StackOverflow. See their [top-20 ranking for June 2016](http://redmonk.com/sogrady/2016/07/20/language-rankings-6-16/) as an example.\n",
    "\n",
    "\n",
    "## During this question set we will cover:\n",
    "* *Part 1:* Creating a base RDD and loading data\n",
    "* *Part 2:* Counting with aggregations\n",
    "* *Part 3:* Using an inverted index\n",
    "* *Part 4:* Directly ranking with `reduceByKey()`\n",
    "\n",
    "> Note that for reference, you can look up the details of the relevant methods in:\n",
    "> * [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a base RDD and loading data\n",
    "\n",
    "In this part of the lab, we will explore creating a base RDD with `parallelize` and using pair RDDs to count words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Create a base RDD\n",
    "We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18b7e3c5381002c30262abddaef5d7a9",
     "grade": true,
     "grade_id": "sparkcontext_setup",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOU CAN MOST LIKELY IGNORE THIS CELL. This is only of use for running this notebook locally.\n",
    "\n",
    "# THIS CELL DOES NOT NEED TO BE RUN ON DATABRICKS. \n",
    "# Note that Databricks already creates a SparkContext for you, so this cell can be skipped.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext(appName=\"hw\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"spark context started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langsList = ['java', 'python', 'ruby', 'perl', 'scala', 'haskell', 'clojure', 'groovy']\n",
    "langsRDD = sc.parallelize(langsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "print(type(langsRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Capitalize and test\n",
    "\n",
    "Let's use a `map()` transformation to capitalize each string in the base RDD we just created. We'll define a Python function that properly captalizes the word.  Please replace `<FILL IN>` with your solution. After you have defined `capitalize` you can run the cell which contains a test.  If you implementation is correct it will not print out anything; otherwise it will raise an error.\n",
    "\n",
    "This is the general form that exercises will take, except that no example solution will be provided.  Exercises will include an explanation of what is expected, followed by code cells where one cell will have one or more `<FILL IN>` sections.  The cell that needs to be modified will have `# TODO: Replace <FILL IN> with appropriate code` on its first line.  Once the `<FILL IN>` sections are updated and the code is run, the test cell can then be run to verify the correctness of your solution.  The last code cell before the next markdown section will contain the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e94636b600afe5217d1935369db50b72",
     "grade": false,
     "grade_id": "capitalize",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# One way of completing the function\n",
    "def capitalize(word):\n",
    "    # TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "    # return <FILL IN>\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print(capitalize('java'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df37ce7fa1c4421f1c377254b855df91",
     "grade": true,
     "grade_id": "test_capitalize",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load in the testing code and check to see if your answer is correct\n",
    "# If incorrect it will report back '1 test failed' for each failed test\n",
    "# Make sure to rerun any cell you change before trying the test again\n",
    "from nose.tools import assert_equal, assert_true\n",
    "\n",
    "\"\"\"Check that makePlural function makes its input plural by adding an s\"\"\"\n",
    "assert_equal(capitalize('scala'), 'Scala')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) Apply `capitalize` to the base RDD\n",
    "\n",
    "Now pass each item in the base RDD into a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation that applies the `capitalize()` function to each element. And then call the [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) action to see the transformed RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e216cc505c9ffc757a2f81553517da1",
     "grade": false,
     "grade_id": "pluralRDD",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "# capitalizedRDD = langsRDD.map(<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(capitalizedRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5a7fb10593718aad76390d45d14bbeb",
     "grade": true,
     "grade_id": "test_pluralRDD",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that makePlural was applied to base RDD and call to collect returns correct output\"\"\"\n",
    "assert_equal(capitalizedRDD.collect(), ['Java', 'Python', 'Ruby', 'Perl', 'Scala', 'Haskell', 'Clojure', 'Groovy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Pass a `lambda` function to `map`\n",
    "\n",
    "Let's create the same RDD using a `lambda` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88e96dbd14f719734ae3c25d11b80703",
     "grade": false,
     "grade_id": "pluralLambdaRDD",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "# capitalizedLambdaRDD = langsRDD.map(lambda <FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(capitalizedLambdaRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4d8d54e5bf5181fd7e1605a9013666c",
     "grade": true,
     "grade_id": "test_pluralLambdaRDD",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that lambda function applied to base RDD and call to collect returns correct output\"\"\"\n",
    "assert_equal(capitalizedLambdaRDD.collect(),  ['Java', 'Python', 'Ruby', 'Perl', 'Scala', 'Haskell', 'Clojure', 'Groovy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) Load a text file\n",
    "\n",
    "To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply a `parse()` function using a `map()` transformation to parse a line of the dataset and turn it into a dictionary object containing the article name and title.  Since the file is large we use `first()`, so that we only print 1 line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this code\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/17-700/data/master/hw1/wikipedia.dat\"\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "\n",
    "def parse(line):\n",
    "    subs = \"</title><text>\"\n",
    "    i = line.index(subs)\n",
    "    return { 'title': line[14:i], 'text': line[i+len(subs):len(line)-16] }\n",
    "\n",
    "wikipediaRdd = sc.textFile(\"file://\" + SparkFiles.get(\"wikipedia.dat\"), 8).map(parse)\n",
    "wikipediaRdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Counting with aggregations\n",
    "\n",
    "We will use a simple metric for determining the popularity of a programming language: the number of Wikipedia articles that mention the language at least once.\n",
    "\n",
    "An approach you might first consider (we'll see shortly that there are better ways) is based on using the [aggregate()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.aggregate) transformation. As the name implies, the `aggregate()` transformation aggregates the elements of each partition, and then the results for all the partitions, using the given combine functions and a neutral “zero value.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Computing `occurencesOfLang()`\n",
    "\n",
    "Start by implementing a helper method `occurrencesOfLang` which computes the number of articles that mention the given language at least once. For the sake of simplicity we check that it least one word (delimited by spaces) of the article text is equal to the given language.\n",
    "\n",
    "*Hint: You can use the mentionsLanguage function defined at the beginning of the following cell*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d4c34865950df79da6d5ea72b4f98d4",
     "grade": false,
     "grade_id": "occurencesOfLang",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mentionsLanguage(lang, article):\n",
    "    return lang in article['text'].split(' ')\n",
    "\n",
    "# TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "# initialValue = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def seqOp(lang, count, article):\n",
    "    \"\"\"\n",
    "    Counts the number of language occurences in a partition and returns the running accumulated result.\n",
    "    Args:\n",
    "        lang (str): language that we are looking for\n",
    "        count (int): running tally of the number of times the language has been mentioned so far in the partition\n",
    "        article (dict): article to evaluate for occurences of the language\n",
    "    Returns:\n",
    "        int: updated tally of the number of mentions of the language in the partition\n",
    "    \"\"\"\n",
    "    # TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "    # return <FILL IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def combOp(countA, countB):\n",
    "    \"\"\"\n",
    "    Combines the number of language mentions in two partitions into a single result.\n",
    "    Args:\n",
    "        countA (int): occurences of the language in partition A\n",
    "        countB (int): occurences of the language in partition B\n",
    "    Returns:\n",
    "        int: combined count across both partitions\n",
    "    \"\"\"\n",
    "    # TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "    # return <FILL IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def occurencesOfLang(lang, rdd):\n",
    "    \"\"\"\n",
    "    Returns the number of occurences of a language in a dataset.\n",
    "    Args:\n",
    "        lang (str): language to search for\n",
    "        rdd (rdd): dataset to search against\n",
    "    Returns:\n",
    "        int: number of occurences found in dataset\n",
    "    \"\"\"\n",
    "    return rdd.aggregate(initialValue, lambda x, y: seqOp(lang, x, y), combOp)\n",
    "    \n",
    "print(occurencesOfLang('Java', wikipediaRdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "176316f1d845ac2b9ec0ab46ab158169",
     "grade": true,
     "grade_id": "test_occurencesOfLang",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that occurencesOfLang returns the correct number of occurences for Java\"\"\"\n",
    "assert_equal(occurencesOfLang('Java', wikipediaRdd), 360)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Computing the ranking with `rankLangs()`\n",
    "\n",
    "Using occurrencesOfLang, implement a method rankLangs which computes a list of pairs where the second component of the pair is the number of articles that mention the language (the first component of the pair is the name of the language).\n",
    "\n",
    "An example of what rankLangs might return might look like this, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(\"Scala\", 999999), (\"JavaScript\", 1278), (\"LOLCODE\", 982), (\"Java\", 42)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list should be sorted in descending order. That is, according to this ranking, the pair with the highest second component (the count) should be the first element of the list.\n",
    "\n",
    "Pay attention to roughly how long it takes to run this part! (It should take tens of seconds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27e0ea7385a973ee83aa2436466f4496",
     "grade": false,
     "grade_id": "rankLangs",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def rankLangs(langs, rdd):\n",
    "    \"\"\"\n",
    "    Returns the number of occurences of a language in a dataset.\n",
    "    Args:\n",
    "        langs (list[str]): languages to search for\n",
    "        rdd (rdd): dataset to search against\n",
    "    Returns:\n",
    "        list[str, int]: language counts sorted in descending order\n",
    "    \"\"\"\n",
    "    # TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "    # return <FILL IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "print(rankLangs(['Python', 'Scala', 'JavaScript'], wikipediaRdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d32826beea5d070085e9f18a90a0efd",
     "grade": true,
     "grade_id": "test_rankLangs",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "langs = ['JavaScript', 'Java', 'PHP', 'Python', 'C#', 'C++', 'Ruby', 'CSS', 'Objective-C', 'Perl', 'Scala', 'Haskell', 'MATLAB', 'Clojure', 'Groovy']\n",
    "\n",
    "correctRanking = [('JavaScript', 977), ('C#', 538), ('Java', 360), ('C++', 199), ('CSS', 191), ('PHP', 165), ('Python', 165),\n",
    "                  ('Perl', 86), ('Ruby', 80), ('Objective-C', 32), ('Haskell', 32), ('Scala', 24), ('Clojure', 15),\n",
    "                  ('MATLAB', 14), ('Groovy', 11)]\n",
    "\n",
    "\"\"\"Check that rankLangs returns the correct number of occurences in descending order\"\"\"\n",
    "assert_equal(rankLangs(langs, wikipediaRdd), correctRanking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using an inverted index\n",
    "\n",
    "An inverted index is an index data structure storing a mapping from content, such as words or numbers, to a set of documents. In particular, the purpose of an inverted index is to allow fast full text searches. In our use-case, an inverted index would be useful for mapping from the names of programming languages to the collection of Wikipedia articles that mention the name at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Compute an inverted index\n",
    "\n",
    "Implement the `makeIndex()` method which returns a pair RDD of type ('&lt;lang&gt;', '&lt;[articles]&gt;'). This RDD contains pairs, such that for each language in the given langs list there is at most one pair. Furthermore, the second component of each pair contains the articles that mention the language at least once.\n",
    "\n",
    "Hint: You might want to use methods `flatMap()` and `groupByKey()` on RDD for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fb4861cd8a0e4edd37077f750f807ea",
     "grade": false,
     "grade_id": "invertedIndex",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def makeIndex(langs, rdd):\n",
    "    \"\"\"\n",
    "    Returns the number of occurences of a language in a dataset.\n",
    "    Args:\n",
    "        langs (list[str]): languages to search for\n",
    "        rdd (rdd): dataset to search against\n",
    "    Returns:\n",
    "        list[str, int]: language counts sorted in descending order\n",
    "    \"\"\"\n",
    "    # TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "    # return <FILL IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "print(len(list(makeIndex(['Python'], wikipediaRdd).first()[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b4dc9c34ccb7b2754f14d524abe55aa",
     "grade": true,
     "grade_id": "test_invertedIndex",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that makeIndex returns correct language mappings\"\"\"\n",
    "assert_equal(len(list(makeIndex(['PHP'], wikipediaRdd).first()[1])), 165)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Computing the ranking\n",
    "\n",
    "Use the `makeIndex()` method implemented in the previous part to implement a faster method for computing the language ranking.\n",
    "\n",
    "Like in Part 2, `rankLangsUsingIndex()` should compute a list of pairs where the second component of the pair is the number of articles that mention the language (the first component of the pair is the name of the language).\n",
    "\n",
    "Again, the list should be sorted in descending order. That is, according to this ranking, the pair with the highest second component (the count) should be the first element of the list.\n",
    "\n",
    "Hint: method the `mapValues()` defined for Pair RDDs on could be useful for this part.\n",
    "\n",
    "Can you notice a performance improvement over the attempt in Part 2? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfe201dd22912ac90686cb655d6f7794",
     "grade": false,
     "grade_id": "rankLangsUsingIndex",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def rankLangsUsingIndex(index):\n",
    "    \"\"\"\n",
    "    Returns the number of occurences of a language using the provided index.\n",
    "    Args:\n",
    "        index (rdd): index generated by makeIndex\n",
    "    Returns:\n",
    "        list[str, int]: language counts sorted in descending order\n",
    "    \"\"\"\n",
    "    # TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "    # return <FILL IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "print(rankLangsUsingIndex(makeIndex(['Python', 'Scala', 'JavaScript'], wikipediaRdd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81f0e5df43d4106be19174b180fc0f54",
     "grade": true,
     "grade_id": "test_rankLangsUsingIndex",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that rankLangs returns the correct number of occurences in descending order\"\"\"\n",
    "assert_equal(rankLangsUsingIndex(makeIndex(langs, wikipediaRdd)), correctRanking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Directly ranking with `reduceByKey()`\n",
    "\n",
    "In the case where the inverted index from above is only used for computing the ranking and for no other task (full-text search, say), it is more efficient to use the `reduceByKey()` method to compute the ranking directly, without first computing an inverted index. Note that the `reduceByKey()` method is only defined for RDDs containing pairs (each pair is interpreted as a key-value pair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Implementing `rankLangsReduceByKey()`\n",
    "\n",
    "Implement the `rankLangsReduceByKey()` method, this time computing the ranking without the inverted index, using `reduceByKey()`.\n",
    "\n",
    "Like in Parts 2 and 3, `rankLangsReduceByKey()` should compute a list of pairs where the second component of the pair is the number of articles that mention the language (the first component of the pair is the name of the language).\n",
    "\n",
    "Again, the list should be sorted in descending order. That is, according to this ranking, the pair with the highest second component (the count) should be the first element of the list.\n",
    "\n",
    "Can you notice an improvement in performance compared to measuring both the computation of the index and the computation of the ranking in the previous attempts? If so, can you think of a reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01c032f8851f79cf9f86e7b6771f1798",
     "grade": false,
     "grade_id": "rankLangsReduceByKey",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def rankLangsReduceByKey(langs, rdd):\n",
    "    \"\"\"\n",
    "    Returns the number of occurences of a language in a dataset.\n",
    "    Args:\n",
    "        langs (list[str]): languages to search for\n",
    "        rdd (rdd): dataset to search against\n",
    "    Returns:\n",
    "        list[str, int]: language counts sorted in descending order\n",
    "    \"\"\"\n",
    "    # TODO: Uncomment the template below and replace <FILL IN> with appropriate code\n",
    "    # return <FILL IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "print(rankLangsReduceByKey(['Python', 'Scala', 'JavaScript'], wikipediaRdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61c52762747046030e82ed9c34f1543d",
     "grade": true,
     "grade_id": "test_rankLangsReduceByKey",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that rankLangsReduceByKey returns the correct number of occurences in descending order\"\"\"\n",
    "assert_equal(rankLangsReduceByKey(langs, wikipediaRdd), correctRanking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the notebook as an IPython notebook, then submit it to Gradescope!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "homework1_answer",
  "notebookId": 3903747651600317
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
