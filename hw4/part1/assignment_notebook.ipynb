{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU 17-400/17-700 auto-graded notebook\n",
    "\n",
    "Before you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6hsVlo6xIhi"
   },
   "source": [
    "# Homework 4: Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe60cd9540617cc2e184126ec775bb7a",
     "grade": false,
     "grade_id": "collaborators",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Who did you collaborate with on this assignment? \n",
    "# if no one, collaborators should contain an empty string,\n",
    "# else list your collaborators below\n",
    "\n",
    "# collaborators = [\"\"]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dccedd0e8702e7a99d0ea08f2fa7921",
     "grade": true,
     "grade_id": "test_collaborators",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    collaborators\n",
    "except:\n",
    "    raise AssertionError(\"you did not list your collaborators, if any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook, we will implement a multi-layer perceptron to perform entity classification (i.e., predicting the category label of a DBPedia entity based on its title) without using any deep learning frameworks. The auto-differentiation will be handled by the provided code. You only need to using the existing APIs to build and evaluate a model. \n",
    "\n",
    "First, we review some background knowledge about Multi-layer Perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFM7X6AnxM-6"
   },
   "source": [
    "# Multi-layer Perceptron\n",
    "MLP is a simple neural network architecture consisting of multiple layers, each of which apply a linear transformation to their inputs followed by a non-linear mapping:\n",
    "\\begin{align*}\n",
    "    o_i &= f(x^T w_i + b_i)\n",
    "\\end{align*}\n",
    "\n",
    "Here $x \\in \\mathbb{R}^{d_{in}}$ is the layer input and $o_i \\in \\mathbb{R}$ is the \n",
    "$i$-th output of the layer. $w_i$, $b_i$ are layer parameters which will be optimized\n",
    "during training. The number of outputs at each layer is called the *dimension* of that layer and we denote it by $d_{hid}$. We would like to use vector/matrix multiplications \n",
    "wherever possible to utilize their fast implementation in ${numpy}$, and combine the above\n",
    "for all $i$ as:\n",
    "\\begin{equation*}\n",
    "    o = f(x^T W + b)\n",
    "\\end{equation*}\n",
    "\n",
    "$W=[w_1,w_2,\\ldots,w_{d_{hid}}] \\in \\mathbb{R}^{d_{in} \\times d_{hid}}$ stacks all the weight vectors \n",
    "horizontally, and $b \\in \\mathbb{R}^{d_{hid}}$ holds all the biases. The non-linearity $f$ is\n",
    "applied elementwise.\n",
    "\n",
    "To further speed-up the computation we can process a minibatch of inputs together. Let $X \\in \\mathbb{R}^{N \\times d_{in}}$\n",
    "be a matrix holding $N$ examples row-wise. We can compute the layer outputs for all of these together:\n",
    "\\begin{equation}\n",
    "    O = f(X W + B)\n",
    "\\end{equation}\n",
    "\n",
    "$B = \\mathbf{1} \\otimes b^T \\in \\mathbb{R}^{N \\times d_{hid}}$ is a ``broadcasted`` version of the bias\n",
    "of appropriate dimensions. For this assigment we will use `numpy` for all matrix operations, which takes care\n",
    "of broadcasting automatically (see here https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html for details), hence we can use the vector $b$ directly.\n",
    "\n",
    "The nonlinearity we will use is the `Rectified Linear Unit (ReLU)`:\n",
    "\\begin{equation*}\n",
    "    f(x) = \n",
    "    \\begin{cases}\n",
    "        x &\\quad x>0 \\\\\n",
    "        0 &\\quad x\\leq0\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "For vectors the nonlinearity is applied element-wise, and we can again use numpy broadcasting for\n",
    "this. In multi-layer networks, output of layer $k$ is passed as input to layer $k+1$:\n",
    "\\begin{equation}\n",
    "    O^{(k+1)} = f(O^{(k)}W^{(k)} + b^{(k)})\n",
    "\\end{equation}\n",
    "\n",
    "We can set output size of the last layer of the MLP to produce a vector the same size as the number of\n",
    "labels $C$ in our dataset. The operations described thus far map inputs to positive reals, but for \n",
    "classification tasks we are interested \n",
    "in obtaining a *distribution* over class labels. This is usually done by\n",
    "passing the output of the last layer through a `softMax` operation:\n",
    "\\begin{equation}\n",
    "    p_j = \\frac{e^{o_j}}{\\sum_{j'=1}^C e^{o_{j'}}}\n",
    "\\end{equation}\n",
    "\n",
    "Note that $p$ defines a valid distribution, and elements of $o$ which have a\n",
    "high relative value will have a high probability in $p$. In case $o_j$s are very large or very negative there might be numerical issues in computing the above. A more numerically stable version of softMax uses the following:\n",
    "\n",
    "\\begin{equation}\n",
    "    p_j = \\frac{e^{o_j-a}}{\\sum_{j'=1}^C e^{o_{j'}-a}} \n",
    "\\end{equation}\n",
    "\n",
    "This is true for any $a$; we will use $a=\\max_j o_j$. Lastly, we need to define a loss function which measures how far the output  distribution $p_i$ for input $i$ is from its target distribution $t_i$. We will use the cross-entropy loss for this:\n",
    "\\begin{equation}\n",
    "    l_i = - \\sum_{j=1}^C t_{i}^{(j)} \\log p_{i}^{(j)} \n",
    "\\end{equation}\n",
    "For single-label classification, $t_i$ is a $C$-dimensional one-hot vector encoding the correct label for this example. The above equations compute $p$ and $l$ for a single $o$, but in your code you should use `numpy`\n",
    "operations to compute a minibatch of distributions $P$ and losses $L$ from a minibatches of $O$.\n",
    "The objective function we will optimize is the average of losses across a minibatch:\n",
    "\\begin{equation}\n",
    "    \\text{loss} = \\frac{1}{N} \\sum_{i=1}^N l_i \n",
    "\\end{equation}\n",
    "Now we can take gradients of $\\text{loss}$ wrt to the parameters of the network and perform\n",
    "Stochastic Gradient Descent (SGD).\n",
    "\n",
    "To summarize, the architecture you will implement for this assignment consists of a MLP\n",
    "with one hidden layer, followed by a softMax layer and cross-entropy loss.\n",
    "Given an input minibatch $X$ and their associated targets $T$, the output $P$ and loss is computed as:\n",
    "\\begin{align*}\n",
    "    O^{(1)} &= \\text{relu}(X W^{(1)} + b^{(1)}) \\\\\n",
    "    O^{(2)} &= \\text{relu}({O^{(1)}} W^{(2)} + b^{(2)}) \\\\\n",
    "    P &= \\text{softMax}(O^{(2)}) \\\\\n",
    "    \\text{loss} &= \\text{mean}(\\text{crossEnt}(P,T))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ct68KVvQ34iS"
   },
   "source": [
    "# Start: Download and read through the helper functions\n",
    "We provide the following files to help implement a multi-layer perceptron easier\n",
    "* `xman.py` -- classes for expression manager, registers and operations.\n",
    "* `utils.py` -- classes for data preprocessing and forming minibatches.\n",
    "* `functions.py` -- function definitions and their gradients are declared here.\n",
    "* `autograd.py` -- class for performing forward and backward propagation over a Wengert list.\n",
    "\n",
    "*Next, we will download these files and the small train/val/test data we will use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the helper functions and raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEkT5l_uiHcJ"
   },
   "outputs": [],
   "source": [
    "# Just run this cell\n",
    "# Load dependencies and data for this assignment\n",
    "!wget https://raw.githubusercontent.com/17-700/data/master/hw4/autodiff_dependencies/autograd.py\n",
    "!wget https://raw.githubusercontent.com/17-700/data/master/hw4/autodiff_dependencies/functions.py\n",
    "!wget https://raw.githubusercontent.com/17-700/data/master/hw4/autodiff_dependencies/utils.py\n",
    "!wget https://raw.githubusercontent.com/17-700/data/master/hw4/autodiff_dependencies/xman.py\n",
    "!wget https://raw.githubusercontent.com/17-700/data/master/hw4/autodiff_dependencies/data/tiny.train\n",
    "!wget https://raw.githubusercontent.com/17-700/data/master/hw4/autodiff_dependencies/data/tiny.valid\n",
    "!wget https://raw.githubusercontent.com/17-700/data/master/hw4/autodiff_dependencies/data/tiny.test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gef6mf7Z5uOF"
   },
   "source": [
    "## Understand the declared operations and their gradient computation in the helper classes\n",
    "We have declared all the primitive operations in the `XManFunctions` class for you. Please read the `functions.py` file carefully. Here are some explanations that help you understand the code.\n",
    "\n",
    "    import numpy as np\n",
    "    # forward pass\n",
    "    EVAL_FUNS = {\n",
    "            'add':      lambda x1,x2: x1+x2,\n",
    "    }\n",
    "        \n",
    "    def _derivAdd(delta,x1):\n",
    "        if delta.shape!=x1.shape:\n",
    "            # broadcast, sum along axis=0\n",
    "            return delta.sum(axis=0)\n",
    "        else: \n",
    "            return delta\n",
    "\n",
    "    # backward pass\n",
    "    BP_FUNS = {\n",
    "            'add':    \t[lambda delta,out,x1,x2: _derivAdd(delta,x1),    \n",
    "                        lambda delta,out,x1,x2: _derivAdd(delta,x2)],\n",
    "    }\n",
    "\n",
    "`EVAL_FUNS` is a dictionary whose keys are the names of the operators as declared in the previous section and values are the actual functions themselves (usually defined using lambda calculus). `BP_FUNS` is another dictionary with the same set of keys as `EVAL_FUNS`, but whose value for a key is a list of functions each computing its gradient wrt one of its inputs. \n",
    "\n",
    "In the above example `BP_FUNS['add'][0]` computes the derivative wrt `x1`, and `BP_FUNS['add'][1]` computes the derivative wrt `x2`. As input each of these functions receives:\n",
    "* `delta` - partial derivative of the output of this operation\n",
    "* `out` - output of this operation in the forward pass. This can be sometimes useful for computing the derivative. For example, for the sigmoid nonlinearity $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$.\n",
    "* `x1,x2,...` - all inputs to the operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement three key primitive operations (1) forward pass, (2) backward pass, and (3) optimization/model updating, using the provided APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from xman import *\n",
    "from utils import *\n",
    "from autograd import *\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "EPS=1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWCY-QXIgsQL"
   },
   "source": [
    "# Implement the Forward / Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d828687e03ac1357c29899675cf05547",
     "grade": false,
     "grade_id": "forward_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: define the forward pass function using Autograd\n",
    "\n",
    "def fwd(network, valueDict):\n",
    "    \"\"\"\n",
    "        network: the MLP object, use network.my_xman to get the MLP model\n",
    "        valueDict: dict where the keys are the name for the data (e.g., 'x', 'y') and parameters, \n",
    "                    and the values are the corresponding data/parameter values\n",
    "        return: loss that you want to compute gradients on\n",
    "    \"\"\"\n",
    "    \n",
    "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "    # # hints: need to use `network.my_xman.operationSequence' defined in xman.py to get the loss register in ad.val()\n",
    "    # ad = Autograd(network.my_xman)\n",
    "    # return ad.eval(<FILL IN>)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cee34ecb5c64914655ef772b1a8184b4",
     "grade": false,
     "grade_id": "backward_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: define the backward pass function using Autograd\n",
    "\n",
    "def bwd(network, valueDict):\n",
    "    \"\"\"\n",
    "        network: the MLP object, use network.my_xman to get the MLP model\n",
    "        valueDict: dict where the keys are the names for the data (e.g., 'x', 'y') and parameters, \n",
    "                    and the values are the corresponding data/parameter values\n",
    "        return: gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "    # # important: see bprop() defined in autograd.py \n",
    "    # ad = Autograd(network.my_xman)\n",
    "    # return ad.bprop(<FILL IN>)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gqpg1ca63ECU"
   },
   "source": [
    "# Implement Optimization\n",
    "\n",
    "Parameters of the above network can be trained using minibatch SGD. Once the loss function is defined we can take its derivative wrt any parameter $w_{ij}$ and update it as follows:\n",
    "\\begin{equation}\n",
    "    w_{ij}^{(k)} \\leftarrow w_{ij}^{(k-1)} - \\lambda \\frac{\\partial{\\text{loss}}}{w_{ij}}\n",
    "\\end{equation}\n",
    "$\\lambda$ is the learning rate.\n",
    "In this assignment, you are not required to modify the learning rate as the training proceeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bef5821f094a3dd4b944b0799a90d924",
     "grade": false,
     "grade_id": "update_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement the optimization step (i.e., applying the gradients)\n",
    "\n",
    "def update(network, dataParamDict, grads, rate):\n",
    "    \"\"\"\n",
    "        network: the MLP object, use network.my_xman to get the MLP model\n",
    "        dataParamDict: dict of parameter values (key: paramter names)\n",
    "        grads: dict that contains gradients of parameters (key: parameter names)\n",
    "        rate: learning rate, a scalar value\n",
    "        return: the updated dataParamDict\n",
    "    \"\"\"\n",
    "\n",
    "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "    # for rname in grads:\n",
    "    #     if network.my_xman.isParam(rname):\n",
    "    #         <FILL IN>\n",
    "    # return dataParamDict\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ToIAL6IJ8FAl"
   },
   "source": [
    "# Build the Model\n",
    "Once the primitive operations are defined, we can go ahead and define the model. First we need to declare registers to hold inputs and parameters. Suppose there is one input *x*, a target *y* and two parameters *W* and *b*:\n",
    "\n",
    "    W = f.param(name='W', default=a*np.random.uniform(-1.,1.,(10,10))\n",
    "    b = f.param(name='b', default=0.1*np.random.uniform(-1.,1.,(10,))\n",
    "    x = f.input(name='x', default=np.random.rand(1,10))\n",
    "    y = f.input(name='y', default=np.random.rand(1,10))\n",
    "\n",
    "We will specify the `name` and `default` fields for each register, including `input` registers! The `name` is used during the forward and backward passes to bind values to the correct register indexed by their names. The `default` value is used as initialization for parameters and also for performing gradient checks. We will use the `inputDict` method described in the next section to collect values for all registers and perform gradient checking using that. For this purpose, you can assign any reasonable random default value to the `input` registers (e.g., don't make them all zeros) with the right shape. \n",
    "\n",
    "**Note on initialization of parameters**: It is important to initialize parameters such that intermediate values in the network do not lie in the saturated regions of the non-linearity. One good heuristic is to sample the weights for $W$ of size $d_{in} \\times d_{out}$ from a uniform distribution $\\mathcal{U}[-a,a]$ whose scale $a$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\sqrt[]{\\frac{6}{d_{in} + d_{out}}}\n",
    "\\end{equation}\n",
    "\n",
    "This is called Glorot initialization (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf). Bias terms can be initialized at a scale of 0.1.\n",
    "\n",
    "Now write the model in terms of primitive operations:\n",
    "\n",
    "    xm = XMan()\n",
    "    xm.o1 = f.relu( f.mul(x,W) + b )\n",
    "    ...\n",
    "    xm.loss = ...\n",
    "    my_xman = xm.setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "gD8L9sRMgpPv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3aa08396e0d67484f487abfed28b518",
     "grade": false,
     "grade_id": "mlp_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def glorot(m,n):\n",
    "    # return scale for glorot initialization\n",
    "    return np.sqrt(6./(m+n))\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron\n",
    "    Accepts list of layer sizes [in_size, hid_size1, hid_size2, ..., out_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        \n",
    "        # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "        # self.num_layers = <FILL IN>\n",
    "    \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        self.my_xman = self._build(layer_sizes) # DO NOT REMOVE THIS LINE. Store the output of xman.setup() in this variable       \n",
    "\n",
    "    def _build(self, layer_sizes):\n",
    "        \n",
    "        \n",
    "        # TODO: Define your model here\n",
    "        \n",
    "        # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "        # self.params = {}\n",
    "        # for i in range(self.num_layers):\n",
    "        #     k = i+1\n",
    "        #     sc = glorot(layer_sizes[i], layer_sizes[i+1])\n",
    "        #     self.params['W'+str(k)] = f.param(name='W'+str(k), default=<FILL IN>)\n",
    "        #     self.params['b'+str(k)] = f.param(name='b'+str(k), default=<FILL IN>)\n",
    "      \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        self.inputs = {}\n",
    "        \n",
    "        # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "        # self.inputs['X'] = f.input(name='X', default=<FILL IN>)\n",
    "        # self.inputs['y'] = f.input(name='y', default=<FILL IN>)\n",
    "        # x = XMan()\n",
    "        # inp = self.inputs['X']\n",
    "        # for i in range(self.num_layers):\n",
    "        #     <FILL IN>\n",
    "        # x.output = <FILL IN>\n",
    "        # x.loss = f.mean(f.crossEnt(<FILL IN>))\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x.setup()\n",
    "\n",
    "    \n",
    "    def data_dict(self, X, y):\n",
    "        dataDict = {}\n",
    "        dataDict['X'] = X\n",
    "        dataDict['y'] = y\n",
    "        return dataDict\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABWoEWu4g3Bv"
   },
   "source": [
    "## Data Format and Data Loading\n",
    "For this assignment you need to predict the category label of a DBPedia entity based on its title. The data contains two columns separated by\n",
    "tab, with the title in the first column and label in second.\n",
    "\n",
    "    Lloyd_Stinson   Person\n",
    "    Lobogenesis_centrota    Species\n",
    "    Loch_of_Craiglush   Place\n",
    "\n",
    "\n",
    "*We have done (almost) all data pre-processing in the provided code. The following are some details regarding that.*\n",
    "\n",
    "We will encode entities for input to the networks by converting characters to a one-hot representation. Suppose we have a dictionary mapping each character in the data to an index `chardict = {'a':1,'b':2...}` and the total number of characters in the dataset is $V$, then we will represent `'a'` as a $V$-dimensional vector $[1,0,0,\\ldots,0]$, and `'b'` as another $V$-dimensional vector $[0,1,0,\\ldots,0]$. A string of characters will be encoded to a matrix whose each row is a $V$-dimensional vector.\n",
    "\n",
    "We will fix the maximum length of an entity to $M$, longer entities will be truncated to this length, and shorter ones will be padded with white-space. We have provided you with code that preprocesses the data and divides it into minibatches in `utils.py`. You can,\n",
    "\n",
    "    from utils import *\n",
    "    # load data and preprocess\n",
    "    dp = DataPreprocessor()\n",
    "    data = dp.preprocess(<training_file>, <validation_file>, \n",
    "        <testing_file>)\n",
    "    # minibatches\n",
    "    mb_train = MinibatchLoader(data.training, batch_size, max_len, \n",
    "            len(data.chardict), len(data.labeldict))\n",
    "    mb_valid = MinibatchLoader(data.validation, len(data.validation), \n",
    "            max_len, len(data.chardict), len(data.labeldict), \n",
    "            shuffle=False)\n",
    "    mb_test = MinibatchLoader(data.test, len(data.test), max_len, \n",
    "            len(data.chardict), len(data.labeldict), shuffle=False)\n",
    "\n",
    "`max_len` is the maximum length $M$ which we set. `shuffle=True/False` tells the batch loader whether to shuffle the data after every epoch. For validation and test sets we set the batch size same as the size of the dataset. You can then iterate over the data using,\n",
    "\n",
    "    for (idxs,e,l) in mb_train:\n",
    "        # idxs - ids of examples in minibatch\n",
    "        # e - entities in one-hot format\n",
    "        # l - corresponding output labels also in one-hot format\n",
    "\n",
    "After every epoch (full sweep through `mb_train`) the data is shuffled for the next epoch in `mb_train`. `idxs` has shape $N$, e has shape $N \\times M \\times V$ and l has shape $N \\times C$ where $N$ is the batch size. Make sure that this makes sense to you.\n",
    "\n",
    "For input to the MLP we will concatenate all the one-hot encodings into one row vector, so you will need to flatten `e` to a $N \\times MV$ size matrix whose each row consists of the encoding of all characters in the entity one after the other. You can use `numpy.reshape` for this. \n",
    "\n",
    "The `Data` and `MiniBatchLoader` classes create dictionaries for all characters and labels in the dataset and use that to encode the inputs and labels into a one-hot vector format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bffc5fd15ce8e73c4571043ff9e433a",
     "grade": false,
     "grade_id": "train_epoch_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: prepare the input and do a fwd-bckwd pass over it and update the weights\n",
    "# by calling fwd(), bwd(), and update() we implemented previously\n",
    "\n",
    "def train_epoch(train_dataset, mlp, lr, value_dict, logger):\n",
    "    \"\"\"\n",
    "        train_dataset: data for train\n",
    "        mlp: MLP object\n",
    "        lr: learning rate\n",
    "        value_dict: dict where the keys are the names for the data (e.g., 'x', 'y') and parameters, \n",
    "                    and the values are the corresponding data/parameter values\n",
    "        logger: file object for logging the training loss\n",
    "        return: training loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "    # train_loss = np.ndarray([0])\n",
    "    # for ii, (idxs,e,l) in enumerate(train_dataset):      \n",
    "    #     data_dict = mlp.data_dict(e.reshape(<FILL IN>),l)\n",
    "    #     for k,v in data_dict.items():\n",
    "    #         value_dict[k] = v\n",
    "    #     # fwd-bwd\n",
    "    #     vd = <FILL IN>\n",
    "    #     gd = <FILL IN>\n",
    "    #     value_dict = update(<FILL IN>)\n",
    "    #     message = 'TRAIN loss = %.3f' % vd['loss']\n",
    "    #     logger.write(message+'\\n')\n",
    "    #     \n",
    "    #     train_loss = np.append(train_loss, vd['loss'])\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a80a83de167e752ad9274abff55b7533",
     "grade": false,
     "grade_id": "validate_epoch_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: prepare the input and do a fwd pass over it to compute the loss\n",
    "\n",
    "def validate(valid_dataset, mlp, value_dict):\n",
    "    \"\"\"\n",
    "        valid_dataset: data for validation\n",
    "        mlp: MLP object\n",
    "        value_dict: dict where the keys are the names for the data (e.g., 'x', 'y') and parameters, \n",
    "                    and the values are the corresponding data/parameter values\n",
    "    \"\"\"\n",
    "       \n",
    "    # tot_loss is the sum of loss over all data, n is the number of data points\n",
    "    # probs are the list of output probabilities, targets are a list of labels\n",
    "    tot_loss, n= 0., 0\n",
    "    probs = []\n",
    "    targets = []\n",
    "    \n",
    "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "    # for (idxs,e,l) in valid_dataset: \n",
    "    #     data_dict = mlp.data_dict(e.reshape(<FILL IN>),l)\n",
    "    #     for k,v in data_dict.items():\n",
    "    #         value_dict[k] = v\n",
    "    #     # fwd\n",
    "    #     vd = <FILL IN>\n",
    "    #     tot_loss += <FILL IN>\n",
    "    #     probs.append(<FILL IN>)\n",
    "    #     targets.append(<FILL IN>)\n",
    "    #     n += 1     \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return tot_loss, probs, targets, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93f9a7287e696afe52db0d762bf6c19a",
     "grade": false,
     "grade_id": "test_epoch_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: prepare input and do a fwd pass over it to compute the output probs\n",
    "\n",
    "def test(test_dataset, mlp, best_param_dict):\n",
    "    \"\"\"\n",
    "        test_dataset: data for test\n",
    "        mlp: MLP object\n",
    "        best_param_dict: dict contains learned parameter values with best training performance\n",
    "    \"\"\"\n",
    "    \n",
    "     # tot_loss is the sum of loss over all data, n is the number of data points\n",
    "    # probs are the list of output probabilities, targets are a list of labels\n",
    "    tot_loss, n= 0., 0\n",
    "    probs = []\n",
    "    targets = []\n",
    "    \n",
    "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "    # for (idxs,e,l) in test_dataset: \n",
    "    #     data_dict = mlp.data_dict(e.reshape(<FILL IN>),l)\n",
    "    #     for k,v in data_dict.items():\n",
    "    #         best_param_dict[k] = v\n",
    "    #     # fwd\n",
    "    #     vd = <FILL IN>\n",
    "    #     tot_loss += <FILL IN>\n",
    "    #     probs.append(<FILL IN>)\n",
    "    #     targets.append(<FILL IN>)\n",
    "    #     n += 1   \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return tot_loss, probs, targets, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(probs, targets):\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    targ = np.argmax(targets, axis=1)\n",
    "    return float((preds==targ).sum())/preds.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "PYEBrX4jKDd5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30dd3e0ebea8681be4fff8a10f0d82d2",
     "grade": false,
     "grade_id": "main_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Main function doing training and testing\n",
    "\n",
    "def main(params):\n",
    "    # params: dict for all relevant parameters\n",
    "    \n",
    "    epochs = params['epochs']\n",
    "    max_len = params['max_len']\n",
    "    num_hid = params['num_hid']\n",
    "    batch_size = params['batch_size']\n",
    "    dataset = params['dataset']\n",
    "    init_lr = params['init_lr']\n",
    "    output_file = params['output_file']\n",
    "    train_loss_file = params['train_loss_file']\n",
    "\n",
    "    # load data and preprocess\n",
    "    dp = DataPreprocessor()\n",
    "    data = dp.preprocess('%s.train'%dataset, '%s.valid'%dataset, '%s.test'%dataset)\n",
    "    \n",
    "    # create minibatches\n",
    "    mb_train = MinibatchLoader(data.training, batch_size, max_len,\n",
    "            len(data.chardict), len(data.labeldict))\n",
    "    mb_valid = MinibatchLoader(data.validation, len(data.validation), max_len,\n",
    "            len(data.chardict), len(data.labeldict), shuffle=False)\n",
    "    mb_test = MinibatchLoader(data.test, len(data.test), max_len,\n",
    "            len(data.chardict), len(data.labeldict), shuffle=False)\n",
    "\n",
    "    # build    \n",
    "    mlp = MLP([max_len*mb_train.num_chars, num_hid, mb_train.num_labels])\n",
    "      \n",
    "    logger = open('%s_mlp4c_L%d_H%d_B%d_E%d_lr%.3f.txt'%\n",
    "            (dataset,max_len,num_hid,batch_size,epochs,init_lr),'w')\n",
    "\n",
    "    # get default data and params\n",
    "    value_dict = mlp.my_xman.inputDict()\n",
    "    min_loss = 1e5\n",
    "    lr = init_lr\n",
    "    best_param_dict = {}\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # training\n",
    "        \n",
    "        # # TODO: Uncomment the line below and replace <FILL IN> with appropriate code\n",
    "        # hint: call train_epoch\n",
    "        # train_loss = <FILL IN>\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # validate\n",
    "        tot_loss, probs, targets, n = validate(mb_valid, mlp, value_dict)\n",
    "        \n",
    "        acc = accuracy(np.vstack(probs), np.vstack(targets))\n",
    "        c_loss = tot_loss/n\n",
    "        if c_loss<min_loss:\n",
    "            min_loss = c_loss\n",
    "            for k,v in value_dict.items():\n",
    "                best_param_dict[k] = np.copy(v)\n",
    "        message = ('Epoch %d VAL loss %.3f min_loss %.3f acc %.3f' %\n",
    "                (i,c_loss,min_loss,acc))\n",
    "\n",
    "    np.save(train_loss_file, train_loss)\n",
    "\n",
    "    # testing\n",
    "    tot_loss, probs, targets, n = test(mb_test, mlp, best_param_dict)\n",
    "    acc = accuracy(np.vstack(probs), np.vstack(targets))\n",
    "    c_loss = tot_loss/n\n",
    "    np.save(output_file, np.vstack(probs))\n",
    "    \n",
    "    return c_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good job! Let's train the model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "0_teZLIIgyJb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5abf91bd26ada38ee167c71c57cf2bb",
     "grade": false,
     "grade_id": "helper_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Don't change this cell\n",
    "# Helper functions used for testing training loss \n",
    "EPS = 1e-4\n",
    "\n",
    "LOSS_INV_MAX = 20 \n",
    "\n",
    "MLP_LOSS_THRESHOLD = [(1.022,15), (1.5,10), (2.0, 0)]\n",
    "MLP_TIME_THRESHOLD = [(25,15), (50,10), (100,0)]\n",
    "MLP_LOSS_INV_THRESHOLD = [(LOSS_INV_MAX/2,10), (LOSS_INV_MAX*3/4,5), (LOSS_INV_MAX,0)]\n",
    "\n",
    "def linear(thresholds, x):\n",
    "    return float(thresholds[1][1]-thresholds[0][1])*(x- thresholds[0][0])/(thresholds[1][0]-thresholds[0][0])+thresholds[0][1]\n",
    "\n",
    "def linear_mark(thresholds, x):\n",
    "    if x<=thresholds[0][0]:\n",
    "        return thresholds[0][1]\n",
    "    elif x<=thresholds[1][0]:\n",
    "        return linear(thresholds[:2], x)\n",
    "    elif x<=thresholds[2][0]:\n",
    "        return linear(thresholds[1:3], x)\n",
    "    else:\n",
    "        return thresholds[2][1]    \n",
    "\n",
    "def _crossEnt(x,y):\n",
    "    # X, y: 2-D numpy array\n",
    "    # return: return an array of cross entropy, where each element is crossEnt between x_i and y_i\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def load_params_from_file(filename):\n",
    "    return np.load(filename)[()]\n",
    "\n",
    "def save_params_to_file(d, filename):\n",
    "    np.save(filename, d)    \n",
    "\n",
    "def loss_inv_check(loss_arr):\n",
    "    try:\n",
    "        in_arr = np.reshape(loss_arr, [len(loss_arr)])\n",
    "        if len(in_arr) < LOSS_INV_MAX:\n",
    "            raise ValueError(\"Not enough Train Loss measurements. Found only %d train loss entries!\"%len(loss_arr))\n",
    "        in_arr = in_arr[0:LOSS_INV_MAX]\n",
    "        inv = 0\n",
    "        for i in range(LOSS_INV_MAX-1):\n",
    "            if in_arr[i] < in_arr[i+1]:\n",
    "                inv += 1\n",
    "        \n",
    "        return inv\n",
    "    except Exception as e:\n",
    "        print (\"MLP TRAIN LOSS FAILED\" )\n",
    "        print (e)  \n",
    "        return -1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10ee3a9b44b38d8c701899fe2c550153",
     "grade": true,
     "grade_id": "helper_function_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 1.60>_crossEnt(np.array([[0.1,0.1,0.8]]), np.array([[0.33,0.33,0.33]])).mean()>1.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cca85d503c1d92f91e927be8ee6d8c1a",
     "grade": true,
     "grade_id": "hyperparameters_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset and parameters for testing\n",
    "# Do not tune the hyper-parameters below (though in practice you should)\n",
    "\n",
    "params = dict()\n",
    "params['max_len'] = 10\n",
    "params['num_hid'] = 50\n",
    "params['batch_size'] = 64\n",
    "params['dataset'] = 'tiny'\n",
    "params['epochs'] = 50\n",
    "params['init_lr'] = 0.1\n",
    "params['output_file'] = 'output'\n",
    "params['train_loss_file'] = 'train_loss'\n",
    "\n",
    "# make sure didn't change the hyper-parameters\n",
    "assert params['init_lr'] == 0.1\n",
    "assert params['epochs'] == 50\n",
    "assert params['max_len'] == 10\n",
    "assert params['num_hid'] == 50\n",
    "assert params['batch_size'] == 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EVt26V_GQ0n"
   },
   "outputs": [],
   "source": [
    "# Load the data and parameters for testing\n",
    "\n",
    "epochs = params['epochs']\n",
    "max_len = params['max_len']\n",
    "num_hid = params['num_hid']\n",
    "batch_size = params['batch_size']\n",
    "dataset = params['dataset']\n",
    "init_lr = params['init_lr']\n",
    "output_file = params['output_file']\n",
    "train_loss_file = params['train_loss_file']\n",
    "\n",
    "# load data and preprocess\n",
    "dp = DataPreprocessor()\n",
    "data = dp.preprocess('%s.train'%dataset, '%s.valid'%dataset, '%s.test'%dataset)\n",
    "\n",
    "# minibatches\n",
    "mb_train = MinibatchLoader(data.training, batch_size, max_len,\n",
    "        len(data.chardict), len(data.labeldict))\n",
    "mb_test = MinibatchLoader(data.test, len(data.test), max_len,\n",
    "        len(data.chardict), len(data.labeldict), shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2985a3848090b9c2529d828746e36eb1",
     "grade": true,
     "grade_id": "load_data_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure we've correctly loaded data\n",
    "\n",
    "assert mb_train.num_examples == 1857\n",
    "assert mb_test.num_examples == 221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "9oU-EiK7Ixvu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d91e00ef6d78d08ad900916569d6aa72",
     "grade": true,
     "grade_id": "gradient_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testing gradient correctness\n",
    "\n",
    "targets = []\n",
    "indices = []\n",
    "for (idxs,e,l) in mb_test:\n",
    "    targets.append(l)\n",
    "    indices.extend(idxs)\n",
    "\n",
    "mlp = MLP([max_len*mb_train.num_chars,num_hid,mb_train.num_labels])\n",
    "    \n",
    "# function which takes a network object and checks gradients\n",
    "dataParamDict = mlp.my_xman.inputDict()\n",
    "fd = fwd(mlp, dataParamDict)\n",
    "grads = bwd(mlp, fd)\n",
    "for rname in grads:\n",
    "    if mlp.my_xman.isParam(rname):\n",
    "        fd[rname].ravel()[0] += EPS\n",
    "        fp = fwd(mlp, fd)\n",
    "        a = fp['loss']\n",
    "        fd[rname].ravel()[0] -= 2*EPS\n",
    "        fm = fwd(mlp, fd)\n",
    "        b = fm['loss']\n",
    "        fd[rname].ravel()[0] += EPS\n",
    "        auto = grads[rname].ravel()[0]\n",
    "        num = (a-b)/(2*EPS)\n",
    "        if not np.isclose(auto, num, atol=1e-3):\n",
    "            raise ValueError(\"gradients not close for %s, Auto %.5f Num %.5f\"\n",
    "                        % (rname, auto, num))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zmp5JrFxKqbf"
   },
   "outputs": [],
   "source": [
    "# Testing training loss\n",
    "t_start = time.clock()\n",
    "\n",
    "t_start1 = os.times()[0]\n",
    "params[\"output_file\"] = output_file+\"_mlp\"\n",
    "\n",
    "loss, accu = main(params)\n",
    "mlp_time = time.clock()-t_start\n",
    "user_time = os.times()[0]-t_start1\n",
    "\n",
    "student_mlp_loss = _crossEnt(np.load(params[\"output_file\"]+\".npy\"), np.vstack(targets)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa66a0370a8e7e2c0abdd573de65bab9",
     "grade": true,
     "grade_id": "train_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert accu > 0.5"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "autodiff.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
